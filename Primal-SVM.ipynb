{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primal Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X \\in R^{n \\times d+1}$ and $y = (y_1,...,y_n)^T \\in R^{n+1}$ and $\\texttt{loss}(...) \\ge 0$\n",
    "\n",
    "Objective function: \n",
    "$$F(w) = \\|w\\|^2 + \\frac Cn \\|\\texttt{loss}(y,Xw)\\|_1$$\n",
    "\n",
    "###### For Square Hinge Loss:\n",
    "$$l_{square-hinge}(y,t) := \\max(0, 1 - yt)^2$$\n",
    "Then\n",
    "$$\\frac{d}{dt}l_{square-hinge}(y,t) := \\begin{cases} 0, & \\mbox{if } 1-yt \\lt 0\\\\ \n",
    "2(1 - yt)(-y), & \\mbox{if } 1-yt \\gt 0  \\end{cases}$$\n",
    "And\n",
    "$$F(w) = \\|w\\|^2 + \\frac Cn \\sum^n\\max(0, 1 - y*(Xw))^2$$\n",
    "And $\\texttt{for j = 1,2,...,d+1}$\n",
    "$$(\\vec\\nabla F(w))_j = \\begin{cases} 2w_j, & \\mbox{if } 1-y*(Xw) \\le 0\\\\\n",
    "2w_j + \\frac Cn \\sum^{n}_{i=1} 2(1 - y*(Xw))(-y) \\cdot X_{i,j}, & \\mbox{if } 1-y*(Xw) \\gt 0  \\end{cases}$$\n",
    "Where $y*(Xw) \\in R^{n}$<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Implementation with squared hinge loss using Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import randint\n",
    "import random\n",
    "import math\n",
    "from math import exp\n",
    "import time \n",
    "\n",
    "\n",
    "def kernel(xi,xj): # const\n",
    "#     sigma = 8\n",
    "#     return exp(-(norm(xi-xj)**2)/(2*(sigma**2))) # Gaussian\n",
    "    return np.dot(xi,xj)\n",
    "\n",
    "\n",
    "def loss(y,t): # const\n",
    "    return pow(max(0,1-(y*t)),2)\n",
    "\n",
    "\n",
    "def loss_der(y,t): # const\n",
    "    if 1-(y*t) <= 0:\n",
    "        return 0\n",
    "    return 2*(1-(y*t))*(-y)\n",
    "\n",
    "\n",
    "def H(C,X,sv): # const\n",
    "    d = len(X[0])\n",
    "    summation = np.zeros((d,d))\n",
    "    for i in sv:\n",
    "        summation += np.dot(X[i],X[i].T)\n",
    "    return np.eye(d) + C*summation\n",
    "\n",
    "\n",
    "def H_inv(C,X,sv): # const\n",
    "    return np.linalg.inv(H(C,X,sv))\n",
    "\n",
    "\n",
    "def compute_obj(w,C,X,y): # const\n",
    "    ret = 0.0\n",
    "    assert len(X)==len(y)\n",
    "    assert len(X[0])==len(w)\n",
    "    for i in range(len(X)):\n",
    "        ret += loss(y[i],kernel(X[i],w))\n",
    "    return norm(w)**2 + C*ret\n",
    "\n",
    "\n",
    "def compute_grad(w,C,X,sv,y): # const\n",
    "    n=len(sv)\n",
    "    X[len(X)-1,len(w)-1]\n",
    "    grad = 2*w.copy()\n",
    "    for i in range(n):\n",
    "        assert len(X) > sv[i]\n",
    "        grad += 2*C*(np.dot(w,X[sv[i]])-y[sv[i]])*X[sv[i]]\n",
    "    return grad\n",
    "\n",
    "\n",
    "def numer_grad(w,ep,delta,C,X,y): # const\n",
    "    return (compute_obj(w+(ep*delta),C,X,y) \\\n",
    "           -compute_obj(w-(ep*delta),C,X,y))/(2*ep)\n",
    "\n",
    "\n",
    "def grad_checker(w0,C,X,y): # const\n",
    "    ep=.0001\n",
    "    delta=0\n",
    "    d=len(w0)\n",
    "    w=[]\n",
    "    for i in range(d):\n",
    "        delta=np.zeros(w0.shape)\n",
    "        delta[i] = 1\n",
    "        w.append(numer_grad(w0,ep,delta,C,X,y))\n",
    "    return np.asarray(w)\n",
    "\n",
    "def my_gradient_descent(X,y,sv,w0=None,initial_step_size=.1,max_iter=1000,C=1,\n",
    "                        X_test=None, y_test=None,descent_type=''): # const\n",
    "    tol=10**-4 # scikit learn default\n",
    "    if w0 == None:\n",
    "        w0 = np.zeros(len(X[0]))\n",
    "    if len(X) == 0:\n",
    "        return 'Error'\n",
    "    diff = -1\n",
    "    grad = -1\n",
    "    \n",
    "    w = w0\n",
    "    obj_array = []\n",
    "\n",
    "    training_error_array = []\n",
    "    training_error_array.append(score(X, y, w=w))\n",
    "\n",
    "    testing_error_array = []\n",
    "    testing_error_array.append(score(X_test, y_test, w=w))\n",
    "    \n",
    "    w_array = []\n",
    "    w_array.append(w.copy())\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        print 'i',i\n",
    "        obj=compute_obj(w,C,X,y)\n",
    "        print 'obj',obj\n",
    "        obj_array.append(obj)\n",
    "        w_p = w\n",
    "#         if descent_type == 'stochastic':\n",
    "#             random_index = randint(1,len(X))\n",
    "#             grad = compute_grad(w,C,X[random_index],y[random_index])\n",
    "#         else:\n",
    "        grad = compute_grad(w,C,X,sv,y)\n",
    "#             assert norm(grad-grad_checker(w,C,X,y)) < tol\n",
    "        print 'grad',norm(grad)\n",
    "#         print 'grad_checker',norm(grad_checker(w,C,X,y))\n",
    "        if norm(grad) < tol:\n",
    "            break\n",
    "        \n",
    "#         step_size = initial_step_size\n",
    "#         if back_track:\n",
    "#             while obj <= compute_obj(w - (step_size * grad),C,X,y):\n",
    "#                 step_size = step_size/2.0\n",
    "#         print 'step_size',step_size\n",
    "#         w += - step_size * grad\n",
    "        gamma = 0.9\n",
    "        step= - gamma * np.dot(H_inv(C,X,sv), grad)\n",
    "        print 'step',norm(step)\n",
    "        w += step\n",
    "        print 'w',norm(w)\n",
    "\n",
    "        w_array.append(w.copy())\n",
    "        training_error_array.append(score(X, y, w=w))\n",
    "        testing_error_array.append(score(X_test, y_test, w=w))\n",
    "        \n",
    "        if obj*10 < compute_obj(w,C,X,y):\n",
    "            break\n",
    "            \n",
    "        diff = norm(w-w_p)\n",
    "    if norm(grad) > tol:\n",
    "        print 'Warning: Did not converge.'\n",
    "    return w, w_array, obj_array, training_error_array, testing_error_array\n",
    "    \n",
    "def score(X, y, w): # const\n",
    "    error = 0.0\n",
    "    error_comp = 0.0\n",
    "    for i in range(len(X)):\n",
    "        prediction = np.sign(kernel(w,X[i]))\n",
    "        if prediction == 1 and y[i] == 1:\n",
    "            error += 1\n",
    "        elif (prediction == -1 or prediction == 0) and y[i] == -1:\n",
    "            error += 1\n",
    "        else:\n",
    "            error_comp += 1\n",
    "    return 'correct',error/len(X), 'failed',error_comp/len(X)\n",
    "\n",
    "\n",
    "def my_svm(X_train, y_train,sv,max_iter=None,C=None,X_test=None, y_test=None): # const\n",
    "    w0=np.zeros(len(X_train[0]))\n",
    "    w, w_array, obj_array, training_error_array, testing_error_array = \\\n",
    "                    my_gradient_descent(X_train, y_train,sv,w0=w0, max_iter=max_iter,C=C,X_test=X_test, y_test=y_test)\n",
    "    return w, w_array, obj_array, training_error_array, testing_error_array\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "with open('zip.train', 'r') as f:\n",
    "    for line in f:\n",
    "        line_split = line.split()\n",
    "        y.append(float(line_split[0]))\n",
    "        X.append(line_split[1:])\n",
    "X = np.asarray(X)\n",
    "X = X.astype(np.float32, copy=False)\n",
    "y = np.asarray(y)\n",
    "\n",
    "# append constant dimension\n",
    "# X = np.column_stack((X, np.ones(X.shape[0])))\n",
    "\n",
    "y[y < 4.5] = -1\n",
    "y[y >= 4.5] = 1\n",
    "\n",
    "assert len(X)==len(y);assert len(X[y==-1])==len(y[y==-1]);assert len(X[y==1])==len(y[y==1]);\n",
    "\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "with open('zip.test', 'r') as f:\n",
    "    for line in f:\n",
    "        line_split = line.split()\n",
    "        y_test.append(float(line_split[0]))\n",
    "        X_test.append(line_split[1:])\n",
    "X_test = np.asarray(X_test)\n",
    "X_test = X_test.astype(np.float32, copy=False)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "y_test[y_test < 4.5] = -1\n",
    "y_test[y_test >= 4.5] = 1\n",
    "\n",
    "X_train, ignore, y_train, ignore = train_test_split(X, y, train_size=0.05, random_state=20140210)\n",
    "print X_train.shape\n",
    "assert len(X_train)>1;assert len(X_test)>1;assert len(X_train)==len(y_train);assert len(X_test)==len(y_test)\n",
    "\n",
    "max_iter=100\n",
    "C=1.0\n",
    "\n",
    "sv = range(len(X_train))\n",
    "w, w_array, obj_array, training_error_array, testing_error_array = my_svm(X_train, y_train, sv, max_iter=max_iter,C=C,\n",
    "                                                                          X_test=X_test, y_test=y_test)\n",
    "print 'Custom w =',norm(w),' test score = ',score(X_test, y_test, w=w)\n",
    "\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(C=C)\n",
    "clf.fit(X_train, y_train)\n",
    "print 'SVC',' test score = ',clf.score(X_test, y_test)\n",
    "\n",
    "clf = SGDClassifier(loss='squared_hinge', penalty=\"l2\",alpha=1/C, fit_intercept=False)\n",
    "clf.fit(X_train, y_train); assert clf.intercept_ == 0\n",
    "print 'SGDClassifier w = ',norm(clf.coef_[0]),' test score = ',clf.score(X_test, y_test)\n",
    "#score(X_test, y_test,w=clf.coef_[0])\n",
    "\n",
    "clf = LinearSVC(loss='squared_hinge', penalty=\"l2\",C=C, fit_intercept=False); clf.fit(X_train, y_train)\n",
    "assert clf.intercept_ == 0\n",
    "print 'LinearSVC w = ',norm(clf.coef_[0]),' test score = ',clf.score(X_test, y_test)\n",
    "#score(X_test, y_test, w=clf.coef_[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "w_array = np.asarray(w_array)\n",
    "ax.scatter(w_array[:,0],w_array[:,1],marker='^')\n",
    "\n",
    "w_stoch_array = np.asarray(w_stoch_array)\n",
    "ax.scatter(w_stoch_array[:,0],w_stoch_array[:,1],marker='*')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.title('First Two Dimensions of Hyperplane over iterations')\n",
    "plt.ylabel('w [1]')\n",
    "plt.xlabel('w [0]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "obj_array = np.asarray(obj_array)\n",
    "ax.scatter(range(1,len(obj_array)+1),obj_array,marker='^')\n",
    "\n",
    "obj_stoch_array = np.asarray(obj_stoch_array)\n",
    "ax.scatter(range(1,len(obj_stoch_array)+1),obj_stoch_array,marker='*')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.title('Objective over iterations')\n",
    "plt.ylabel('F (w)')\n",
    "plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "training_error_array = np.asarray(training_error_array)\n",
    "testing_error_array = np.asarray(testing_error_array)\n",
    "ax.scatter(range(1,len(training_error_array)+1),training_error_array[:,1],marker='^',label='training error')\n",
    "ax.scatter(range(1,len(testing_error_array)+1),testing_error_array[:,1],marker='*',label='testing error')\n",
    "\n",
    "training_error_stoch_array = np.asarray(training_error_stoch_array)\n",
    "testing_error_stoch_array = np.asarray(testing_error_stoch_array)\n",
    "ax.scatter(range(1,len(training_error_stoch_array)+1),training_error_stoch_array[:,1],marker='@',\n",
    "           label='training error')\n",
    "ax.scatter(range(1,len(testing_error_stoch_array)+1),testing_error_stoch_array[:,1],marker='#',label='testing error')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Classification Error over iterations')\n",
    "plt.ylabel('Classification Error')\n",
    "plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "# print w\n",
    "x_plot=[w[0], 0]\n",
    "y_plot=[w[1], 0]\n",
    "ax.plot(x_plot,y_plot)\n",
    "\n",
    "# print clf.coef_[0]\n",
    "x_plot=[clf.coef_[0][0], 0]\n",
    "y_plot=[clf.coef_[0][1], 0]\n",
    "ax.plot(x_plot,y_plot)\n",
    "\n",
    "ax.scatter((X[y==0])[:,0],(X[y==0])[:,1],marker='*')\n",
    "ax.scatter((X[y==1])[:,0],(X[y==1])[:,1],marker='^')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.title('Data, Scikit-learn Hyperplane, and Our Own Hyperplane')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
