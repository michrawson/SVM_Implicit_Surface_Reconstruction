{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal Slab SVM\n",
    "Let $K \\in R^{m \\times m}$ and $K_{ij} = \\texttt{kernel}(x_i,x_j)$ and $K_{i}$ the $i^{th}$ column of $K$\n",
    "\n",
    "Then Primal Minimization Objective:\n",
    "$$\\min_{\\beta \\in R^m} \\beta^T K \\beta + \\frac{1}{\\nu m} \\sum_i \\texttt{loss}(K_i^T \\beta)$$\n",
    "\n",
    "Let $F$ be the objective function.\n",
    "$$F(\\beta) = \\beta^T K \\beta + \\frac{1}{\\nu m} \\sum_i \\texttt{loss}(K_i^T \\beta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient:\n",
    "$$\\vec\\nabla F(\\beta) = 2K\\beta + \\frac {1}{\\nu m} \\sum_i K_i \\circ \\frac{d}{d\\beta}\\texttt{loss}(K_i^T \\beta)$$\n",
    "\n",
    "Hessian:\n",
    "$$H = 2K + \\frac {1}{\\nu m} \\sum_i \\left( K_i \\circ K_i \\right) \\circ \\frac{d^2}{(d\\beta)^2}\\texttt{loss}(K_i^T \\beta)$$\n",
    "\n",
    "We consider losses:\n",
    "$$\\texttt{loss}_{hinge}(t) = \\max(~0,~ |~\\rho - t~| - \\delta ~)$$\n",
    "$$\\texttt{loss}_{square-hinge}(t) = \\max(~0,~ |~\\rho - t~| - \\delta ~)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Gradients:\n",
    "$$\\frac{d}{dt}\\texttt{loss}_{hinge}(t) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "-1, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "1, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d}{dt}\\texttt{loss}_{square-hinge}(t) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "-2\\left(\\rho-t-\\delta\\right), & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "2\\left(-\\rho+t-\\delta\\right), & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Hessians:\n",
    "$$\\frac{d^2}{(dt)^2}\\texttt{loss}_{hinge}(t) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "0, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "0, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d^2}{(dt)^2}\\texttt{loss}_{square-hinge}(t) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "2, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "2, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "Evaluation:\n",
    "$$ \\langle \\Phi(x), w\\rangle = \\sum_k \\beta_k k(x_k, x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg, random, ones, zeros\n",
    "from numpy.linalg import norm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import mosek\n",
    "import math\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time \n",
    "from collections import namedtuple\n",
    "\n",
    "v=.01\n",
    "delta = 0.1\n",
    "sigma = 100\n",
    "rho = 1\n",
    "max_iter = 100\n",
    "initial_step_size = .1\n",
    "timer_thresh = .1\n",
    "ep = .000001\n",
    "# loss_type = 'hinge'\n",
    "loss_type = 'square-hinge'\n",
    "method = ''\n",
    "x=np.zeros((0))\n",
    "K=np.zeros((0))\n",
    "K_inv=np.zeros((0))\n",
    "points_count = 1000\n",
    "points_std_from_surface = 0.001\n",
    "\n",
    "def H(beta,loss_vect_list):\n",
    "    start = time.time()\n",
    "    ret = 2*K\n",
    "    if loss_type == 'hinge':\n",
    "        pass\n",
    "    elif loss_type == 'square-hinge':\n",
    "        ret += 2/(v*len(x))*np.sum(K[:,loss_vect_list]*K[:,loss_vect_list], axis=0)\n",
    "    else:\n",
    "        for i in range(len(ret)):\n",
    "            ret += 1/(v*len(x))*K[:,i]*K[:,i]*loss_der_der(np.dot(K[:,i],beta))\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'H:',end - start\n",
    "    return ret\n",
    "\n",
    "def loss_der_der(t):\n",
    "    if loss_type == 'hinge':\n",
    "        return 0\n",
    "    if loss_type == 'square-hinge':\n",
    "        if abs(rho - t) < delta:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2\n",
    "    raise Exception(loss_type,t,rho,delta)\n",
    "\n",
    "def loss_der(t):\n",
    "    if loss_type == 'hinge':\n",
    "        if abs(rho - t) <= delta:\n",
    "            return 0\n",
    "        if rho - t > delta:\n",
    "            return -1\n",
    "        if -rho + t > delta:\n",
    "            return 1\n",
    "    if loss_type == 'square-hinge':\n",
    "        if abs(rho - t) <= delta:\n",
    "            return 0\n",
    "        if rho - t > delta:\n",
    "            return -2*(rho - t - delta)\n",
    "        if -rho + t > delta:\n",
    "            return 2*(-rho + t - delta)\n",
    "    raise Exception(loss_type,t,rho,delta)\n",
    "        \n",
    "def obj_grad(beta):\n",
    "    start = time.time()\n",
    "    ret = 2*np.dot(K,beta)\n",
    "    for i in range(len(ret)):\n",
    "        ret += 1/(v*len(x))*K[:,i]*loss_der(np.dot(K[:,i],beta))\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'obj_grad:',end - start\n",
    "    return ret\n",
    "\n",
    "def kernel(x1,x2):\n",
    "    return math.exp(-1*math.pow(norm(x1-x2),2\n",
    "                               )/(2*math.pow(sigma, 2)))\n",
    "\n",
    "def kernel_vect(x_list,x2):\n",
    "    return np.exp(-1*np.power(norm(x_list-x2,axis=1),2 )/(2*math.pow(sigma, 2)))\n",
    "\n",
    "def loss(t):\n",
    "    if loss_type == 'hinge':\n",
    "        return max(0, abs(rho - t) - delta )\n",
    "    if loss_type == 'square-hinge':\n",
    "        return max(0, abs(rho - t) - delta )**2\n",
    "\n",
    "def obj_funct(beta):\n",
    "    start = time.time()\n",
    "    m = len(x)\n",
    "    loss_sum = 0\n",
    "    for i in range(len(x)):\n",
    "        loss_sum += loss(np.dot(K[:,i],beta))\n",
    "    obj = 1/2*np.dot(beta.T,np.dot(K,beta)) + 1 / (v*m) * loss_sum\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'obj_funct:',end - start\n",
    "    return obj\n",
    "\n",
    "def f(x_test, beta):\n",
    "    start = time.time()\n",
    "    \n",
    "    w = np.dot(beta,kernel_vect(x,x_test)) - rho\n",
    "            \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'f:',end - start\n",
    "    return w\n",
    "\n",
    "def step(beta,step_size,resid):\n",
    "    return beta - (step_size * resid)\n",
    "\n",
    "def backtrack_step_size(step_size,obj,resid,beta):\n",
    "    start = time.time()\n",
    "    if step_size == .00000001:\n",
    "        step_size = initial_step_size\n",
    "    elif method == 'Newton':\n",
    "        pass\n",
    "    else:\n",
    "        step_size *= 2.0\n",
    "    while obj < obj_funct( step(beta,step_size,resid) ):\n",
    "        step_size = step_size * 0.6\n",
    "        if step_size < .00000001:\n",
    "            step_size = .00000001\n",
    "            end = time.time()\n",
    "            if end - start > timer_thresh:\n",
    "                print 'backtrack_step_size:',end - start\n",
    "            return step_size\n",
    "\n",
    "    assert obj >= obj_funct( step(beta,step_size,resid) )\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'backtrack_step_size:',end - start\n",
    "    return step_size\n",
    "\n",
    "def numer_grad(beta,ep,delta): # const\n",
    "    return (obj_funct(beta+(ep*delta)) \\\n",
    "           -obj_funct(beta-(ep*delta)))/(2*ep)\n",
    "\n",
    "\n",
    "def grad_checker(beta,ep): # const\n",
    "    d=len(beta)\n",
    "    w=np.zeros(d)\n",
    "    for i in range(d):\n",
    "        direct=np.zeros(beta.shape)\n",
    "        direct[i] = 1\n",
    "        w[i]=(numer_grad(beta,ep,direct))\n",
    "    return w\n",
    "\n",
    "def get_resid(beta,grad,loss_vect_list):\n",
    "    start = time.time()\n",
    "    \n",
    "    if loss_type == 'square-hinge':\n",
    "        resid = linalg.solve(\n",
    "            2*K + 2/(v*len(x))*np.sum(K[:,loss_vect_list]*K[:,loss_vect_list], \n",
    "                                      axis=0),\n",
    "                             grad)\n",
    "    else:\n",
    "        resid = linalg.solve(H(beta,loss_vect_list),grad)\n",
    "        \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'get_resid:',end - start\n",
    "    return resid\n",
    "\n",
    "\n",
    "def grad_des(x):\n",
    "    start = time.time()\n",
    "    x = x\n",
    "    obj_array = -1*np.ones(max_iter)\n",
    "    obj_grad_array = np.zeros((max_iter))\n",
    "    obj_grad_check_array = np.zeros(max_iter)\n",
    "\n",
    "    beta = zeros(len(x))\n",
    "    step_size = initial_step_size\n",
    "    iterations = 0\n",
    "    for i in range(max_iter):\n",
    "        loss_vect_list = None\n",
    "        for j in range(K.shape[1]):\n",
    "            if abs(rho - np.dot(K[:,j],beta)) >= delta :\n",
    "                if loss_vect_list == None:\n",
    "                    loss_vect_list = np.asarray([ j ])\n",
    "                else:\n",
    "                    loss_vect_list = np.append(loss_vect_list, j)\n",
    "\n",
    "\n",
    "        obj = obj_funct(beta)\n",
    "        obj_array[i]=(obj)\n",
    "\n",
    "        grad = obj_grad(beta)\n",
    "        obj_grad_array[i]=norm(grad)\n",
    "        obj_grad_check_array[i]=norm((grad-grad_checker(beta,ep)))\n",
    "\n",
    "        if i>0 and abs(obj_array[i]-obj_array[i-1])<.00001:\n",
    "            break\n",
    "        if obj < .0001:\n",
    "            break\n",
    "        if norm(grad) < ep:\n",
    "            break\n",
    "            \n",
    "        if method == 'Newton':\n",
    "            resid = get_resid(beta,grad,loss_vect_list) # np.dot(H_inv(beta),grad)\n",
    "            step_size = 1\n",
    "        else:\n",
    "            resid = (grad)\n",
    "            \n",
    "        step_size = backtrack_step_size(step_size,obj,resid,beta)\n",
    "        \n",
    "        beta = step(beta,step_size,resid) # Update\n",
    "\n",
    "        if i == max_iter-1:\n",
    "            print 'WARNING: Did not converge'\n",
    "            \n",
    "        iterations += 1\n",
    "\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'grad_des:',end - start\n",
    "    return Run(obj_array,obj_grad_array,obj_grad_check_array,beta,iterations)\n",
    "\n",
    "def get_data_points():\n",
    "    start = time.time()\n",
    "    points = random.random((points_count,2))*2*np.pi\n",
    "\n",
    "    x = np.zeros((len(points),3))\n",
    "    for p in range(len(points)):\n",
    "        r = random.normal(loc=1,scale=points_std_from_surface)\n",
    "        z_cord = r * np.sin(points[p][1])\n",
    "\n",
    "        r_temp = r * np.cos(points[p][1])\n",
    "        y_cord = r_temp * np.sin(points[p][0])\n",
    "        x_cord = r_temp * np.cos(points[p][0])\n",
    "\n",
    "        x[p] = np.asarray([x_cord, y_cord, z_cord])\n",
    "            \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'get_data_points:',end - start\n",
    "    return x\n",
    "\n",
    "def pre_comp_K():\n",
    "    start = time.time()\n",
    "    K=np.zeros((len(x),len(x)))\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x)):\n",
    "            K[i,j] = kernel(x[i],x[j])\n",
    "            \n",
    "    if linalg.cond(K) < 1/sys.float_info.epsilon:\n",
    "        K_inv = linalg.inv(K)\n",
    "    else:\n",
    "        K_inv = np.identity(K.shape[0])\n",
    "            \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'pre_comp_K:',end - start\n",
    "    return K, K_inv\n",
    "\n",
    "x = get_data_points()        \n",
    "        \n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2])\n",
    "plt.show()\n",
    "\n",
    "K, K_inv = pre_comp_K()    \n",
    "\n",
    "Run = namedtuple('Run', ['obj_array','obj_grad_array','obj_grad_check_array','beta',\n",
    "                         'iterations'])\n",
    "    \n",
    "method = 'Newton'\n",
    "Newton_Desc = grad_des(x)\n",
    "print 'Newton Desc iterations',Newton_Desc.iterations\n",
    "\n",
    "method = ''\n",
    "Steepest_Desc = grad_des(x)\n",
    "print 'Steepest Desc iterations',Steepest_Desc.iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_steps = 25\n",
    "\n",
    "def pop_data_grid(beta):\n",
    "    start = time.time()\n",
    "    data = np.zeros((grid_steps,grid_steps,grid_steps))\n",
    "\n",
    "    x0_range = np.linspace(-2, 2, grid_steps)\n",
    "    x1_range = np.linspace(-2, 2, grid_steps)\n",
    "    x2_range = np.linspace(-2, 2, grid_steps)\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'alloc mem:',end - start\n",
    "\n",
    "    for i in range(grid_steps):\n",
    "        for j in range(grid_steps):\n",
    "            for k in range(grid_steps):\n",
    "                data[i,j,k] = f(np.asarray([x0_range[i],\n",
    "                                x1_range[j],\n",
    "                                x2_range[k]]), beta)\n",
    "                \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'pop_data_grid:',end - start\n",
    "    return data\n",
    "\n",
    "def proc_results(beta):\n",
    "    start = time.time()\n",
    "\n",
    "    data = pop_data_grid(beta)\n",
    "\n",
    "    print 'np.abs(data - delta) < .1 -> ',(np.where(np.abs(data - delta) < .1)[0].shape)\n",
    "    print 'np.abs(data - delta) < .01 -> ',(np.where(np.abs(data - delta) < .01)[0].shape)\n",
    "    print 'np.abs(data - delta) < .001 -> ',(np.where(np.abs(data - delta) < .001)[0].shape)\n",
    "    print 'np.abs(data - delta) < .0001 -> ',(np.where(np.abs(data - delta) < .0001)[0].shape)\n",
    "    print 'data < delta -> ',(np.where(data < delta )[0].shape)\n",
    "    print 'data > delta -> ',(np.where(data > delta )[0].shape)\n",
    "    print 'data < 0 -> ',(np.where( data < 0)[0].shape)\n",
    "    print 'data == 0 -> ',(np.where( data == 0)[0].shape)\n",
    "    print 'data > 0 -> ',(np.where( data > 0)[0].shape)\n",
    "    print 'min -> ',(np.amin( data ))\n",
    "    print 'max -> ',(np.amax( data ))\n",
    "    print 'data:',data\n",
    "    \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'proc_results:',end - start\n",
    "\n",
    "proc_results(Newton_Desc.beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.scatter(range(1,Newton_Desc.iterations+1),\n",
    "           Newton_Desc.obj_array[0:Newton_Desc.iterations],marker='^',\n",
    "           label='Non-Stochastic Newtons Method')\n",
    "ax.scatter(range(1,Steepest_Desc.iterations+1),\n",
    "           Steepest_Desc.obj_array[0:Steepest_Desc.iterations],marker='*',\n",
    "           label='Non-Stochastic Steepest Descent')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Objective Function over iterations')\n",
    "plt.ylabel('F (w)')\n",
    "plt.xlabel('Iteration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.scatter(range(1,(Newton_Desc.iterations)+1),\n",
    "           Newton_Desc.obj_grad_array[0:Newton_Desc.iterations],\n",
    "           marker='^',\n",
    "           label='Non-Stochastic Newtons Method')\n",
    "\n",
    "ax.scatter(range(1,(Steepest_Desc.iterations)+1),\n",
    "           Steepest_Desc.obj_grad_array[0:Steepest_Desc.iterations],\n",
    "           marker='*', \n",
    "           label='Non-Stochastic Steepest Descent')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Gradient Norm over iterations')\n",
    "plt.ylabel('norm(d/dw F (w))')\n",
    "plt.xlabel('Iteration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.scatter(range(1,(Steepest_Desc.iterations)+1),\n",
    "           Steepest_Desc.obj_grad_check_array[0:Steepest_Desc.iterations],\n",
    "           marker='*',\n",
    "           label='Non-Stochastic Steepest Descent')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Gradient Norm and Approx. Gradient Norm Difference \\n over iterations')\n",
    "plt.ylabel('Difference')\n",
    "plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
