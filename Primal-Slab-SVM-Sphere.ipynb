{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal Slab SVM\n",
    "Let $K \\in R^{m \\times m}$ and $K_{ij} = \\texttt{kernel}(x_i,x_j)$ and $K_{i}$ the $i^{th}$ column of $K$\n",
    "\n",
    "Evaluation:\n",
    "$$ \\langle \\Phi(x), w\\rangle = \\sum_k \\beta_k k(x_k, x) $$\n",
    "\n",
    "Then Primal Minimization Objective:\n",
    "$$\\min_{\\beta \\in R^m,\\rho \\in R} \\beta^T K \\beta + \\frac{1}{\\nu m} \\sum_i \\texttt{loss}(\\langle \\Phi(x), w\\rangle, \\rho) - \\rho$$\n",
    "$$\\min_{\\beta \\in R^m,\\rho \\in R} \\beta^T K \\beta + \\frac{1}{\\nu m} \\sum_i \\texttt{loss}(K_i^T \\beta, \\rho) - \\rho$$\n",
    "\n",
    "Let $F$ be the objective function.\n",
    "$$F(\\beta,\\rho) = \\beta^T K \\beta + \\frac{1}{\\nu m} \\sum_i \\texttt{loss}(K_i^T \\beta, \\rho) - \\rho$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients:\n",
    "$$\\vec\\nabla_\\beta F(\\beta,\\rho) = 2K\\beta + \\frac {1}{\\nu m} \\sum_i K_i \\circ \\frac{d}{d\\beta}\\texttt{loss}(K_i^T \\beta, \\rho)$$\n",
    "$$\\nabla_\\rho F(\\beta,\\rho) = \\frac {1}{\\nu m} \\sum_i \\frac{d}{d\\rho}\\texttt{loss}(K_i^T \\beta, \\rho)-1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hessians:\n",
    "$$H_\\beta = 2K + \\frac {1}{\\nu m} \\sum_i \\left( K_i \\circ K_i \\right) \\circ \\frac{d^2}{(d\\beta)^2}\\texttt{loss}(K_i^T \\beta, \\rho)$$\n",
    "$$H_\\rho = \\frac {1}{\\nu m} \\sum_i \\frac{d^2}{(d\\rho)^2}\\texttt{loss}(K_i^T \\beta, \\rho)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider losses:\n",
    "$$\\texttt{loss}_{hinge}(t,\\rho) = \\max(~0,~ |~\\rho - t~| - \\delta ~)$$\n",
    "$$\\texttt{loss}_{square-hinge}(t,\\rho) = \\max(~0,~ |~\\rho - t~| - \\delta ~)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Gradients:\n",
    "$$\\frac{d}{dt}\\texttt{loss}_{hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "-1, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "1, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d}{dt}\\texttt{loss}_{square-hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "-2\\left(\\rho-t-\\delta\\right), & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "2\\left(-\\rho+t-\\delta\\right), & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{d\\rho}\\texttt{loss}_{hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "1, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "1, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d}{d\\rho}\\texttt{loss}_{square-hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "2\\left(\\rho-t-\\delta\\right), & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "-2\\left(-\\rho+t-\\delta\\right), & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Hessians:\n",
    "$$\\frac{d^2}{(dt)^2}\\texttt{loss}_{hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "0, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "0, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d^2}{(dt)^2}\\texttt{loss}_{square-hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "2, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "2, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d^2}{(d\\rho)^2}\\texttt{loss}_{hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "0, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "0, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d^2}{(d\\rho)^2}\\texttt{loss}_{square-hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "2, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "2, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:\n",
    "$$ \\langle \\Phi(x), w\\rangle = \\sum_k \\beta_k k(x_k, x) $$\n",
    "Surface:\n",
    "$$ \\langle \\Phi(x), w\\rangle -\\rho = \\sum_k \\beta_k k(x_k, x) -\\rho $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg, random, ones, zeros, matrix, eye, dot\n",
    "from numpy.linalg import norm,cholesky,inv\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import mosek\n",
    "import math\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time \n",
    "from collections import namedtuple\n",
    "\n",
    "v=.001\n",
    "delta = 0.0\n",
    "sigma = 10\n",
    "initial_rho = 1\n",
    "max_iter = 100\n",
    "initial_step_size = .1\n",
    "timer_thresh = .1\n",
    "ep = .00000001\n",
    "points_count = 1000\n",
    "points_std_from_surface = 0\n",
    "\n",
    "def incomplete_cholesky_decomp(K,G):\n",
    "    start = time.time()\n",
    "\n",
    "    assert K.shape[0] == K.shape[1]\n",
    "    n = K.shape[0]\n",
    "\n",
    "    k=-1\n",
    "\n",
    "#     G[0:n,0:n] = \n",
    "    np.fill_diagonal(G[0:n,0:n], np.diagonal(K[0:n,0:n]))\n",
    "    \n",
    "    for i in range(n):\n",
    "        if np.sum(np.diagonal(G[i:n,i:n])) > .5:\n",
    "            j = np.argmax(np.diagonal(G[i:n,i:n]))+i\n",
    "\n",
    "            temp = K.T[0:n,i].copy()\n",
    "            K.T[0:n,i] = K.T[0:n,j]\n",
    "            K.T[0:n,j] = temp\n",
    "\n",
    "            temp = K.T[i,0:n].copy()\n",
    "            K.T[i,0:n] = K.T[j,0:n]\n",
    "            K.T[j,0:n] = temp\n",
    "\n",
    "            temp = G[i,0:i+1].copy()\n",
    "            G[i,0:i+1] = G[j,0:i+1]\n",
    "            G[j,0:i+1] = temp\n",
    "\n",
    "            G[i,i] = math.sqrt(K[i,i])\n",
    "\n",
    "            G[i+1:n,i] = 1/G[i,i]*(K[i+1:n,i]- np.dot(G[i+1:n,0:i],G[i,0:i]) )\n",
    "\n",
    "            \n",
    "            np.fill_diagonal(G[i:n,i:n], np.diagonal(K[i:n,i:n]) - np.sum(np.power(G[i:n,0:i],2),axis=1))\n",
    "        else:\n",
    "            k=i\n",
    "            break\n",
    "\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'product_form_cholesky_decomp:', end - start, 'sec'\n",
    "    return G,k\n",
    "\n",
    "\n",
    "# class Primal_Opt:\n",
    "class Slab_SVM:\n",
    "\n",
    "    def H(self,beta,rho,loss_vect_list,opt_on):\n",
    "        start = time.time()\n",
    "\n",
    "    #     assert g_loss_type != 'hinge'\n",
    "\n",
    "        if opt_on=='b':\n",
    "#             ret = 2*self.K\n",
    "#             ret = np.multiply(self.K[:,loss_vect_list],self.K[:,loss_vect_list])\n",
    "#             ret = 2/(v*self.m)*np.sum(np.multiply(self.K[:,loss_vect_list],self.K[:,loss_vect_list]),axis=0)\n",
    "            ret = 2*self.K + 2/(v*self.m)*np.sum(np.multiply(self.K,self.K),axis=0)\n",
    "        elif opt_on == 'rho':\n",
    "            ret = 2/(v*self.m)\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'H:',end - start,'sec'\n",
    "        return ret\n",
    "\n",
    "    def loss_der_der(self,t,rho):\n",
    "        if g_loss_type == 'hinge':\n",
    "            return 0\n",
    "        if g_loss_type == 'square-hinge':\n",
    "            if abs(rho - t) < delta:\n",
    "                return 0\n",
    "            else:\n",
    "                return 2\n",
    "        raise Exception(g_loss_type,t,rho,delta)\n",
    "\n",
    "    def loss_der_vect(self,grad,t,rho,opt_on):\n",
    "#         print 'grad',self.grad.shape\n",
    "        grad.fill(0)\n",
    "        if g_loss_type == 'hinge':\n",
    "            grad[ np.absolute(rho - t) <= delta ] = 0\n",
    "            grad[ rho - t > delta ] = -1\n",
    "            grad[ -rho + t > delta ] = 1\n",
    "            return grad\n",
    "        if g_loss_type == 'square-hinge':\n",
    "            grad[ np.absolute(rho - t) <= delta ] = 0\n",
    "            if opt_on=='b':\n",
    "                grad[ rho - t > delta ] = -2.0*(rho - t[rho - t > delta] - delta)\n",
    "                grad[ -rho + t > delta ] = 2.0*(-rho + t[-rho + t > delta] - delta)\n",
    "                return grad\n",
    "            if opt_on=='rho':\n",
    "                grad[ rho - t > delta ] = 2*(rho - t[rho - t > delta] - delta)\n",
    "                grad[ -rho + t > delta ] = -2*(-rho + t[-rho + t > delta] - delta)\n",
    "                return grad\n",
    "        raise Exception(grad,g_loss_type,t,rho,delta)\n",
    "\n",
    "    def z(self,x1,w,b):\n",
    "#         w = random.normal(0, 1.0/sigma, size=(D,len(x1)))\n",
    "#         b = random.uniform(0,2*np.pi,size=D)\n",
    "        return math.sqrt(2.0/D) * np.cos(np.dot(w, x1) + b)\n",
    "\n",
    "    def kernel(self,x1,x2):\n",
    "        return math.exp(-1*math.pow(norm(x1-x2),2\n",
    "                                   )/(2*math.pow(sigma, 2)))\n",
    "\n",
    "    def kernel_vect(self,x_list,x2):\n",
    "        return np.exp(-1*np.power(norm(x_list-x2,axis=1),2 )/(2*math.pow(sigma, 2)))\n",
    "\n",
    "    def loss_vect(self,t,rho):\n",
    "        if g_loss_type == 'hinge':\n",
    "            return np.maximum(np.zeros(t.shape), np.absolute(rho - t) - delta )\n",
    "        if g_loss_type == 'square-hinge':\n",
    "            return np.power(np.maximum(np.zeros(t.shape), np.absolute(rho - t) - delta ),2)\n",
    "\n",
    "#     def loss_matrix_rho_vect(self,t,rho):\n",
    "#         rho = np.matrix(rho).T\n",
    "#         t = np.matrix(t)\n",
    "#         if g_loss_type == 'hinge':\n",
    "#             return np.maximum(np.zeros((t.shape[0],rho.shape[1])), np.absolute(t - rho) - delta )\n",
    "#         if g_loss_type == 'square-hinge':\n",
    "#             return np.power(np.maximum(np.zeros((t.shape[0],rho.shape[1])), np.absolute(t - rho) - delta ),2)\n",
    "\n",
    "#     def loss_matrix_t_matrix(self,t,rho):\n",
    "#         if g_loss_type == 'hinge':\n",
    "#             return np.maximum(np.zeros(t.shape), np.absolute(t - rho) - delta )\n",
    "#         if g_loss_type == 'square-hinge':\n",
    "#             return np.power(np.maximum(np.zeros(t.shape), np.absolute(t - rho) - delta ),2)\n",
    "\n",
    "    def obj_funct(self,beta,rho):\n",
    "        start = time.time()\n",
    "\n",
    "        obj = 1.0/2*np.dot(beta,np.dot(self.K,beta)) + 1.0 / (v*self.m) * np.sum(\n",
    "                                                                            self.loss_vect(np.dot(self.K,beta),\n",
    "                                                                                           rho))\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'obj_funct:',end - start,'sec'\n",
    "        return obj\n",
    "\n",
    "#     def obj_funct_beta_matrix(self,beta,rho):\n",
    "#         start = time.time()\n",
    "\n",
    "#         obj = 1.0/2*(norm(np.multiply(beta,np.dot(self.K,beta)),axis=0)) \\\n",
    "#                 + 1.0 / (v*self.m) * np.sum(self.loss_matrix_t_matrix(np.dot(self.K,beta),rho),axis=0)\n",
    "\n",
    "#         end = time.time()\n",
    "#         if end - start > timer_thresh:\n",
    "#             print 'obj_funct_beta_matrix:',end - start,'sec'\n",
    "#         return obj\n",
    "\n",
    "#     def obj_funct_rho_vect(self,beta,rho):\n",
    "#         start = time.time()\n",
    "\n",
    "#         obj = 1.0/2*np.dot(beta,np.dot(self.K,beta)) + 1.0 / (v*self.m) * np.sum(\n",
    "#                                                                     self.loss_matrix_rho_vect(np.dot(self.K,beta),\n",
    "#                                                                                                  rho),axis=1)\n",
    "#     #     assert len(obj) == len(rho)\n",
    "\n",
    "#         end = time.time()\n",
    "#         if end - start > timer_thresh:\n",
    "#             print 'obj_funct_rho_vect:',end - start,'sec'\n",
    "#         return obj\n",
    "\n",
    "    def f(self,x_test, beta,rho):\n",
    "        start = time.time()\n",
    "\n",
    "        w = np.dot(beta,self.kernel_vect(self.x_data,x_test)) - rho\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'f:',end - start,'sec'\n",
    "        return w\n",
    "\n",
    "    def step(self,element,step_size,resid):\n",
    "        return element - (step_size * resid)\n",
    "\n",
    "    def backtrack_step_size_rho(self,step_size,obj,resid,beta,rho):\n",
    "        start = time.time()\n",
    "        number_of_steps = 2\n",
    "        if step_size == ep**2:\n",
    "            step_size = initial_step_size\n",
    "        else:\n",
    "            step_size *= 4.0\n",
    "\n",
    "        iter_count=0\n",
    "\n",
    "        if obj > (self.obj_funct( beta, self.step(rho,step_size,resid)) ):\n",
    "            end = time.time()\n",
    "            if end - start > timer_thresh:\n",
    "                print 'backtrack_step_size_rho:',end - start,'sec',iter_count,'times'\n",
    "            return step_size\n",
    "\n",
    "        while True:\n",
    "            iter_count += 1\n",
    "            steps = step_size*np.logspace(-1,-number_of_steps,num = number_of_steps,base=2.0)\n",
    "#             obj_many_steps = np.array(self.obj_funct_rho_vect( beta, self.step(rho,steps,resid))).ravel()\n",
    "            obj_many_steps = np.zeros(steps.shape)\n",
    "            for i in range(number_of_steps):\n",
    "                obj_many_steps[i] = self.obj_funct( beta, self.step(rho,steps[i],resid))\n",
    "\n",
    "            if np.where(obj - obj_many_steps >= 0)[0].shape[0] > 0:\n",
    "                end = time.time()\n",
    "                if end - start > timer_thresh:\n",
    "                    print 'backtrack_step_size_rho:',end - start,'sec',iter_count,'times'\n",
    "                return steps[np.where(obj - obj_many_steps >= 0)[0][0]]\n",
    "\n",
    "            step_size = steps[-1]\n",
    "            if step_size < ep**2:\n",
    "    #             print 'WARNING: step size not found'\n",
    "                step_size = ep**2\n",
    "                end = time.time()\n",
    "                if end - start > timer_thresh:\n",
    "                    print 'backtrack_step_size_rho:',end - start,'sec',iter_count,'times'\n",
    "                return step_size\n",
    "            number_of_steps *= 2\n",
    "\n",
    "        assert False\n",
    "\n",
    "    def backtrack_step_size_beta(self,step_size,obj,resid,beta,rho):\n",
    "        start = time.time()\n",
    "        number_of_steps = 2\n",
    "        if step_size == ep**2:\n",
    "            step_size = initial_step_size\n",
    "        else:\n",
    "            step_size *= 2.0\n",
    "\n",
    "        iter_count=0\n",
    "\n",
    "        if obj > (self.obj_funct( self.step(beta,step_size,resid),rho) ):\n",
    "            end = time.time()\n",
    "            if end - start > timer_thresh:\n",
    "                print 'backtrack_step_size_beta:',end - start,'sec',iter_count,'times'\n",
    "            return step_size\n",
    "\n",
    "        while True:\n",
    "            iter_count += 1\n",
    "            steps = step_size*np.logspace(-1,-number_of_steps,num = number_of_steps,base=2.0)\n",
    "#             obj_many_steps = (self.obj_funct_beta_matrix(np.asarray(self.step(np.matrix(beta),\n",
    "#                                                                     np.matrix(steps).T,\n",
    "#                                                                     np.matrix(resid)).T\n",
    "#                                                               ), \n",
    "#                                                     rho))\n",
    "            obj_many_steps = np.zeros(steps.shape)\n",
    "            for i in range(number_of_steps):\n",
    "                obj_many_steps[i] = self.obj_funct( self.step(beta,steps[i],resid),rho)\n",
    "\n",
    "\n",
    "            if np.where(obj - obj_many_steps >= 0)[0].shape[0] > 0:\n",
    "                end = time.time()\n",
    "                if end - start > timer_thresh:\n",
    "                    print 'backtrack_step_size_beta:',end - start,'sec',iter_count,'times'\n",
    "                return steps[np.where(obj - obj_many_steps >= 0)[0][0]]\n",
    "\n",
    "            step_size = steps[-1]\n",
    "            if step_size < ep**2:\n",
    "    #             print 'WARNING: step size not found'\n",
    "                step_size = ep**2\n",
    "                end = time.time()\n",
    "                if end - start > timer_thresh:\n",
    "                    print 'backtrack_step_size_beta:',end - start,'sec',iter_count,'times'\n",
    "                return step_size\n",
    "            number_of_steps *= 10\n",
    "\n",
    "        assert False\n",
    "\n",
    "    def numer_grad(self,beta,rho,ep,direct=0,opt_on=''): # const\n",
    "        if opt_on == 'rho':\n",
    "            return (obj_funct(beta,rho+ep) \\\n",
    "                   -obj_funct(beta,-rho*ep))/(2*ep)\n",
    "        return (obj_funct(beta+(ep*direct),rho) \\\n",
    "               -obj_funct(beta-(ep*direct),rho))/(2*ep)\n",
    "\n",
    "    def grad_checker(self,beta,rho,ep,opt_on): # const\n",
    "        start = time.time()\n",
    "\n",
    "        if opt_on == 'rho':\n",
    "            return numer_grad(beta,rho,ep,opt_on=opt_on)\n",
    "\n",
    "        d=len(beta)\n",
    "        w=np.zeros(d)\n",
    "        for i in range(d):\n",
    "            direct=np.zeros(beta.shape)\n",
    "            direct[i] = 1\n",
    "            w[i]=(numer_grad(beta,rho,ep,direct=direct,opt_on=opt_on))\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'grad_checker:',end - start,'sec'        \n",
    "        return w\n",
    "    \n",
    "    def timed_solve(self,A,B):\n",
    "        start = time.time()\n",
    "        \n",
    "        ret = linalg.solve(A,B)\n",
    "        \n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'timed_solve:',end - start,'sec'\n",
    "        return ret\n",
    "\n",
    "    def get_resid(self,beta,rho,grad,loss_vect_list,opt_on):\n",
    "        start = time.time()\n",
    "\n",
    "        if opt_on=='b':\n",
    "#             print 'eigvalsh',linalg.eigvalsh(self.K)\n",
    "\n",
    "            incomplete_cholesky,k = incomplete_cholesky_decomp(self.H(beta,rho,loss_vect_list,opt_on),\n",
    "                                                             np.zeros(self.K.shape))\n",
    "#             print 'k',k\n",
    "#             print 'incomplete_cholesky',incomplete_cholesky\n",
    "            \n",
    "            resid = self.timed_solve(incomplete_cholesky,grad)\n",
    "            resid = self.timed_solve(self.H(beta,rho,loss_vect_list,opt_on),grad)\n",
    "        else:\n",
    "            resid = grad/self.H(beta,rho,loss_vect_list,opt_on)\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'get_resid:',end - start,'sec'\n",
    "        return resid\n",
    "\n",
    "    def grad_des_iterate(self,iterations,opt_on='b'):\n",
    "        start = time.time()\n",
    "        loss_vect_list = np.where(np.absolute(self.rho - np.dot(self.K,self.beta)) >= delta)[0]            \n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'find sv:',end - start,'sec'\n",
    "\n",
    "        obj = self.obj_funct(self.beta,self.rho)\n",
    "    #     print 'obj',obj\n",
    "        self.obj_array[iterations]=(obj)\n",
    "\n",
    "        if opt_on == 'b':\n",
    "            self.grad = 2.0*np.dot(self.K,self.beta) + 1.0/(v*self.m)*np.sum(\n",
    "                                                                (self.K * self.loss_der_vect(\n",
    "                                                                                             self.grad_buffer,\n",
    "                                                                                             np.dot(self.K,self.beta),\n",
    "                                                                                             self.rho,\n",
    "                                                                                             opt_on)),\n",
    "                                                                             axis=0)\n",
    "        elif opt_on == 'rho':\n",
    "            self.grad = 1/(v*self.m)*np.sum(self.loss_der_vect(\n",
    "                    self.grad_buffer,\n",
    "                    np.dot(self.K,self.beta),self.rho,              opt_on))\n",
    "\n",
    "        self.obj_grad_array[iterations]=norm(self.grad)\n",
    "    #     obj_grad_check_array[iterations]=norm((grad-grad_checker(beta,rho,ep,opt_on)))\n",
    "\n",
    "        if obj < ep:\n",
    "            print 'Stopping crit: obj small',obj,'opt_on',opt_on\n",
    "            return True\n",
    "        if norm(self.grad) < ep:\n",
    "            print 'Stopping crit: norm(grad) small',norm(self.grad),'opt_on',opt_on\n",
    "            return True\n",
    "\n",
    "        if g_loss_type == 'square-hinge' and g_method == 'Newton':\n",
    "            resid = self.get_resid(self.beta,self.rho,self.grad,loss_vect_list,opt_on)\n",
    "        else:\n",
    "            resid = self.grad\n",
    "\n",
    "        if opt_on == 'rho':\n",
    "            self.step_size_rho = self.backtrack_step_size_rho(self.step_size_rho,obj,resid,self.beta,self.rho)\n",
    "            self.rho = self.step(self.rho,self.step_size_rho,resid) # Update\n",
    "        else:\n",
    "            self.step_size_beta = self.backtrack_step_size_beta(self.step_size_beta,obj,resid,self.beta,self.rho)\n",
    "            self.beta = self.step(self.beta,self.step_size_beta,resid) # Update\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'grad_des_iterate:',end - start,'sec'\n",
    "\n",
    "        return False\n",
    "        return False,beta,rho,step_size_beta,step_size_rho,obj_array,obj_grad_array,obj_grad_check_array\n",
    "\n",
    "    def grad_des(self):\n",
    "        start = time.time()\n",
    "\n",
    "        self.obj_array = -1*np.ones(max_iter)\n",
    "        self.obj_grad_array = np.zeros((max_iter))\n",
    "        self.obj_grad_check_array = np.zeros(max_iter)\n",
    "\n",
    "        self.beta = zeros(self.m)\n",
    "        self.rho = initial_rho\n",
    "\n",
    "        self.grad_buffer = zeros(self.beta.shape)\n",
    "        self.step_size_beta = initial_step_size\n",
    "        self.step_size_rho = initial_step_size\n",
    "        self.iterations = 0\n",
    "        for i in range(max_iter):\n",
    "            \n",
    "            converged_b = self.grad_des_iterate(self.iterations,opt_on='b')\n",
    "\n",
    "            converged_rho = self.grad_des_iterate(self.iterations,opt_on='rho')\n",
    "            \n",
    "            if converged_b and converged_rho:\n",
    "                break\n",
    "\n",
    "            if i == max_iter-1:\n",
    "                print 'WARNING: Did not converge'\n",
    "\n",
    "            self.iterations += 1\n",
    "\n",
    "        end = time.time()\n",
    "#         if end - start > timer_thresh:\n",
    "        print 'grad_des:',end - start,'sec'\n",
    "#         return Run(self.obj_array,self.obj_grad_array,self.obj_grad_check_array,self.beta,self.rho,iterations)\n",
    "\n",
    "    def pop_K(self):\n",
    "        start = time.time()\n",
    "\n",
    "        self.K=np.zeros((self.m,self.m))\n",
    "        if Fourier_Feature:\n",
    "            z_cache = np.zeros((self.m,D))\n",
    "            w = random.normal(0, 1.0/sigma, size=(self.m*D,len(self.x_data[0])))\n",
    "            b = random.uniform(0,2*np.pi,size=self.m*D)\n",
    "            for i in range(self.m):\n",
    "                z_cache[i]=self.z(self.x_data[i],w[i:i+D,:],b[i:i+D])\n",
    "            end = time.time()\n",
    "            if end - start > timer_thresh:\n",
    "                print 'z_cache:',end - start,'sec'\n",
    "\n",
    "            for i in range(self.m):\n",
    "#                 for j in range(self.m):\n",
    "                self.K[i,:] = np.dot(z_cache,z_cache[i])\n",
    "#                         self.z(self.x_data[i]),self.z(self.x_data[j]))\n",
    "        else:\n",
    "            for i in range(self.m):\n",
    "                self.K[i,:] = self.kernel_vect(self.x_data,self.x_data[i])\n",
    "\n",
    "        if Fourier_Feature:\n",
    "            K_test=np.zeros((self.m,self.m))\n",
    "            for i in range(self.m):\n",
    "                K_test[i,:] = self.kernel_vect(self.x_data,self.x_data[i])\n",
    "\n",
    "            print 'Fourier norm diff', norm(K_test-self.K)\n",
    "            \n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'get_K:',end - start,'sec'\n",
    "\n",
    "    def get_K_inv(K):\n",
    "        start = time.time()\n",
    "\n",
    "        K_inv = inv(K)\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'get_K_inv:',end - start,'sec'\n",
    "        return K_inv\n",
    "\n",
    "    def get_K_cond(K):\n",
    "        start = time.time()\n",
    "\n",
    "        K_cond = linalg.cond(K)\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'get_K_cond:',end - start,'sec'\n",
    "        return K_cond\n",
    "\n",
    "    def pre_comp_K():\n",
    "        start = time.time()\n",
    "\n",
    "        K = get_K()    \n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'pre_comp_K:',end - start,'sec'\n",
    "        return K #, K_inv\n",
    "\n",
    "    def __init__(self, x_data):\n",
    "        self.x_data = x_data\n",
    "        self.m = len(self.x_data)\n",
    "        self.pop_K()\n",
    "        self.grad_des()\n",
    "        \n",
    "def get_data_points():\n",
    "    start = time.time()\n",
    "    points = random.random((points_count,2))*2*np.pi\n",
    "\n",
    "    x = np.zeros((points_count,3))\n",
    "    for p in range(points_count):\n",
    "        radius = 1\n",
    "        if points_std_from_surface > 0:\n",
    "            r = random.normal(loc=radius,scale=points_std_from_surface)\n",
    "        else:\n",
    "            r = radius\n",
    "        z_cord = r * np.sin(points[p][1])\n",
    "\n",
    "        r_temp = r * np.cos(points[p][1])\n",
    "        y_cord = r_temp * np.sin(points[p][0])\n",
    "        x_cord = r_temp * np.cos(points[p][0])\n",
    "\n",
    "        x[p] = np.asarray([x_cord, y_cord, z_cord])\n",
    "\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'get_data_points:',end - start,'sec'\n",
    "    return x\n",
    "    \n",
    "g_x = get_data_points()        \n",
    "\n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(g_x[:,0],g_x[:,1],g_x[:,2])\n",
    "plt.show()\n",
    "\n",
    "# Run = namedtuple('Run', ['obj_array','obj_grad_array','obj_grad_check_array','beta','rho',\n",
    "#                          'iterations'])\n",
    "\n",
    "Fourier_Feature = False\n",
    "\n",
    "g_Desc = {}\n",
    "g_counter=0\n",
    "for g_loss_type in ['square-hinge', 'hinge']:\n",
    "    for g_method in ['Newton', '']:\n",
    "\n",
    "        print '-----------------------------------'\n",
    "        print 'loss_type',g_loss_type\n",
    "        print 'method',g_method        \n",
    "        g_Desc[g_counter] = Slab_SVM(g_x)\n",
    "        g_Desc[g_counter].D = 0\n",
    "        print 'Desc iterations',g_Desc[g_counter].iterations\n",
    "        print 'Desc rho',g_Desc[g_counter].rho\n",
    "        g_counter += 1\n",
    "        print '-----------------------------------'\n",
    "        print \n",
    "\n",
    "#         Fourier_Feature = True\n",
    "\n",
    "#         for i in range(13):# [1.,2.,50.,100.,500.,750.,800.,900.,1000.]:\n",
    "#             D = 2**i\n",
    "#             print '-----------------------------------'\n",
    "#             print 'D',D\n",
    "#             print 'loss_type',g_loss_type\n",
    "#             print 'method',g_method        \n",
    "#             g_Desc[g_counter] = Slab_SVM(g_x)\n",
    "#             g_Desc[g_counter].D = D\n",
    "#             print 'Desc iterations',g_Desc[g_counter].iterations\n",
    "#             print 'Desc rho',g_Desc[g_counter].rho\n",
    "#             g_counter += 1\n",
    "#             print '-----------------------------------'\n",
    "#             print \n",
    "        \n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_steps = 25\n",
    "\n",
    "def pop_data_grid(beta,rho):\n",
    "    start = time.time()\n",
    "    data = np.zeros((grid_steps,grid_steps,grid_steps))\n",
    "\n",
    "    x0_range = np.linspace(-2, 2, grid_steps)\n",
    "    x1_range = np.linspace(-2, 2, grid_steps)\n",
    "    x2_range = np.linspace(-2, 2, grid_steps)\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'alloc mem:',end - start\n",
    "\n",
    "    for i in range(grid_steps):\n",
    "        for j in range(grid_steps):\n",
    "            for k in range(grid_steps):\n",
    "                data[i,j,k] = f(np.asarray([x0_range[i],\n",
    "                                x1_range[j],\n",
    "                                x2_range[k]]), beta,rho)\n",
    "                \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'pop_data_grid:',end - start,'sec'\n",
    "    return data\n",
    "\n",
    "def proc_data(beta,rho,data):\n",
    "    start = time.time()\n",
    "\n",
    "    print 'delta',delta\n",
    "    print 'rho',rho\n",
    "    print 'np.abs(data - delta) < .1 -> ',(np.where(np.abs(data - delta) < .1)[0].shape)\n",
    "    print 'np.abs(data - delta) < .01 -> ',(np.where(np.abs(data - delta) < .01)[0].shape)\n",
    "    print 'np.abs(data - delta) < .001 -> ',(np.where(np.abs(data - delta) < .001)[0].shape)\n",
    "    print 'np.abs(data - delta) < .0001 -> ',(np.where(np.abs(data - delta) < .0001)[0].shape)\n",
    "    print 'data < delta -> ',(np.where(data < delta )[0].shape)\n",
    "    print 'data > delta -> ',(np.where(data > delta )[0].shape)\n",
    "    print 'data < 0 -> ',(np.where( data < 0)[0].shape)\n",
    "    print 'data == 0 -> ',(np.where( data == 0)[0].shape)\n",
    "    print 'data > 0 -> ',(np.where( data > 0)[0].shape)\n",
    "    print 'min -> ',(np.amin( data ))\n",
    "    print 'max -> ',(np.amax( data ))\n",
    "#     print 'data:',data\n",
    "    \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'proc_results:',end - start\n",
    "\n",
    "for counter in g_Desc:\n",
    "    rho = g_Desc[counter].rho\n",
    "    beta = g_Desc[counter].beta\n",
    "    print 'obj_funct',g_Desc[counter].obj_funct(beta,rho)\n",
    "    f = g_Desc[counter].f\n",
    "    g_m = len(g_x)\n",
    "\n",
    "    print 'D',g_Desc[counter].D\n",
    "    losses = []\n",
    "    for i in range(g_m):\n",
    "        losses.append(f(g_x[i], beta, rho))\n",
    "    losses = np.asarray(losses)\n",
    "    print 'losses min -> ',(np.amin( losses ))\n",
    "    print 'losses min -> ',(np.argmin( losses ))\n",
    "    print 'losses min -> ',g_x[(np.argmin( losses ))]\n",
    "    print 'losses max -> ',(np.amax( losses ))\n",
    "    print 'losses max -> ',(np.argmax( losses ))\n",
    "    print 'losses max -> ',g_x[(np.argmax( losses ))]\n",
    "\n",
    "        \n",
    "    data = pop_data_grid(beta,rho)\n",
    "    proc_data(beta,rho,data)\n",
    "#     break\n",
    "\n",
    "    %matplotlib inline\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import sys\n",
    "    from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "    from skimage import measure\n",
    "    from skimage.draw import ellipsoid\n",
    "\n",
    "    # Use marching cubes to obtain the surface mesh of these ellipsoids\n",
    "    verts, faces = measure.marching_cubes(data, 0)\n",
    "\n",
    "    # Display resulting triangular mesh using Matplotlib. This can also be done\n",
    "    # with mayavi (see skimage.measure.marching_cubes docstring).\n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Fancy indexing: `verts[faces]` to generate a collection of triangles\n",
    "    mesh = Poly3DCollection(verts[faces])\n",
    "    ax.add_collection3d(mesh)\n",
    "\n",
    "    ax.set_xlabel(\"x-axis\")\n",
    "    ax.set_ylabel(\"y-axis\")\n",
    "    ax.set_zlabel(\"z-axis\")\n",
    "\n",
    "    ax.set_xlim(-0, 30)  \n",
    "    ax.set_ylim(-0, 30)  \n",
    "    ax.set_zlim(-0, 30)  \n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.scatter(range(1,Newton_Desc.iterations+1),\n",
    "           Newton_Desc.obj_array[0:Newton_Desc.iterations],marker='^',\n",
    "           label='Non-Stochastic Newtons Method')\n",
    "ax.scatter(range(1,Steepest_Desc.iterations+1),\n",
    "           Steepest_Desc.obj_array[0:Steepest_Desc.iterations],marker='*',\n",
    "           label='Non-Stochastic Steepest Descent')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Objective Function over iterations')\n",
    "plt.ylabel('F (w)')\n",
    "plt.xlabel('Iteration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.scatter(range(1,(Newton_Desc.iterations)+1),\n",
    "           Newton_Desc.obj_grad_array[0:Newton_Desc.iterations],\n",
    "           marker='^',\n",
    "           label='Non-Stochastic Newtons Method')\n",
    "\n",
    "ax.scatter(range(1,(Steepest_Desc.iterations)+1),\n",
    "           Steepest_Desc.obj_grad_array[0:Steepest_Desc.iterations],\n",
    "           marker='*', \n",
    "           label='Non-Stochastic Steepest Descent')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Gradient Norm over iterations')\n",
    "plt.ylabel('norm(d/dw F (w))')\n",
    "plt.xlabel('Iteration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.scatter(range(1,(Steepest_Desc.iterations)+1),\n",
    "           Steepest_Desc.obj_grad_check_array[0:Steepest_Desc.iterations],\n",
    "           marker='*',\n",
    "           label='Non-Stochastic Steepest Descent')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Gradient Norm and Approx. Gradient Norm Difference \\n over iterations')\n",
    "plt.ylabel('Difference')\n",
    "plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
