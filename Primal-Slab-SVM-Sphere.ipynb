{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal Slab SVM\n",
    "Let $K \\in R^{m \\times m}$ and $K_{ij} = \\texttt{kernel}(x_i,x_j)$ and $K_{i}$ the $i^{th}$ column of $K$\n",
    "\n",
    "Then Primal Minimization Objective:\n",
    "$$\\min_{\\beta \\in R^m,\\rho \\in R} \\beta^T K \\beta + \\frac{1}{\\nu m} \\sum_i \\texttt{loss}(K_i^T \\beta, \\rho)$$\n",
    "\n",
    "Let $F$ be the objective function.\n",
    "$$F(\\beta,\\rho) = \\beta^T K \\beta + \\frac{1}{\\nu m} \\sum_i \\texttt{loss}(K_i^T \\beta, \\rho)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients:\n",
    "$$\\vec\\nabla_\\beta F(\\beta,\\rho) = 2K\\beta + \\frac {1}{\\nu m} \\sum_i K_i \\circ \\frac{d}{d\\beta}\\texttt{loss}(K_i^T \\beta, \\rho)$$\n",
    "$$\\nabla_\\rho F(\\beta,\\rho) = \\frac {1}{\\nu m} \\sum_i \\frac{d}{d\\rho}\\texttt{loss}(K_i^T \\beta, \\rho)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hessians:\n",
    "$$H_\\beta = 2K + \\frac {1}{\\nu m} \\sum_i \\left( K_i \\circ K_i \\right) \\circ \\frac{d^2}{(d\\beta)^2}\\texttt{loss}(K_i^T \\beta, \\rho)$$\n",
    "$$H_\\rho = \\frac {1}{\\nu m} \\sum_i \\frac{d^2}{(d\\rho)^2}\\texttt{loss}(K_i^T \\beta, \\rho)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider losses:\n",
    "$$\\texttt{loss}_{hinge}(t,\\rho) = \\max(~0,~ |~\\rho - t~| - \\delta ~)$$\n",
    "$$\\texttt{loss}_{square-hinge}(t,\\rho) = \\max(~0,~ |~\\rho - t~| - \\delta ~)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Gradients:\n",
    "$$\\frac{d}{dt}\\texttt{loss}_{hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "-1, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "1, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d}{dt}\\texttt{loss}_{square-hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "-2\\left(\\rho-t-\\delta\\right), & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "2\\left(-\\rho+t-\\delta\\right), & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{d\\rho}\\texttt{loss}_{hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "1, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "1, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d}{d\\rho}\\texttt{loss}_{square-hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "2\\left(\\rho-t-\\delta\\right), & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "-2\\left(-\\rho+t-\\delta\\right), & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Hessians:\n",
    "$$\\frac{d^2}{(dt)^2}\\texttt{loss}_{hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "0, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "0, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d^2}{(dt)^2}\\texttt{loss}_{square-hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "2, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "2, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d^2}{(d\\rho)^2}\\texttt{loss}_{hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "0, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "0, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d^2}{(d\\rho)^2}\\texttt{loss}_{square-hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "2, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "2, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:\n",
    "$$ \\langle \\Phi(x), w\\rangle = \\sum_k \\beta_k k(x_k, x) $$\n",
    "Surface:\n",
    "$$ \\langle \\Phi(x), w\\rangle -\\rho = \\sum_k \\beta_k k(x_k, x) -\\rho $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg, random, ones, zeros, matrix\n",
    "from numpy.linalg import norm,cholesky,inv\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import mosek\n",
    "import math\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time \n",
    "from collections import namedtuple\n",
    "\n",
    "v=.00001\n",
    "delta = 0.0\n",
    "sigma = 100\n",
    "initial_rho = 1\n",
    "max_iter = 100\n",
    "initial_step_size = .1\n",
    "timer_thresh = .1\n",
    "ep = .000001\n",
    "x=np.zeros((0))\n",
    "m = len(x)\n",
    "K=np.zeros((0))\n",
    "points_count = 100000\n",
    "points_std_from_surface = 0\n",
    "\n",
    "from numpy import eye,dot\n",
    "\n",
    "def pivot(A,k,n):\n",
    "    y = np.amax(np.absolute(A[k:n+1, k:n+1]),axis=1)\n",
    "    i = np.argmax(np.absolute(A[k:n+1, k:n+1]),axis=1)\n",
    "    piv = np.amax(y)\n",
    "    jpiv = np.argmax(y)\n",
    "    ipiv = i[jpiv]\n",
    "    jpiv = jpiv+k-1;\n",
    "    ipiv = ipiv + k-1;\n",
    "    Pk=eye(n)\n",
    "    Pk[ipiv,ipiv]=0\n",
    "    Pk[k,k]=0\n",
    "    Pk[k,ipiv]=1\n",
    "    Pk[ipiv,k]=1\n",
    "    Qk=eye(n)\n",
    "    Qk[jpiv,jpiv]=0\n",
    "    Qk[k,k]=0\n",
    "    Qk[k,jpiv]=1\n",
    "    Qk[jpiv,k]=1\n",
    "    return Pk,Qk\n",
    "\n",
    "def incomplete_LU_decomp(A):\n",
    "    start = time.time()\n",
    "    \n",
    "    assert A.shape[0] == A.shape[1]\n",
    "    n = A.shape[0]\n",
    "    for k in range(n-1):\n",
    "        Pk,Qk = pivot(A,k,n)\n",
    "        A=dot(dot(Pk,A),Qk)\n",
    "        print A\n",
    "        for i in range(k+1,n):\n",
    "            if A[i,k] != 0:\n",
    "                if A[k,k] == 0:\n",
    "                    return 'Error: Null Pivot'\n",
    "                A[i,k] = A[i,k]/A[k,k]\n",
    "                for j in range(k+1,n):\n",
    "                    if A[i,j] != 0:\n",
    "                        A[i,j] = A[i,j] - (A[i,k]/A[k,j])\n",
    "                        \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'incomplete_LU_decomp:',end - start,'sec'\n",
    "    return A\n",
    "\n",
    "import scipy\n",
    "def get_K_LU():\n",
    "    K_LU = scipy.linalg.cholesky(K, lower=True)\n",
    "    K_LU = cholesky(K)\n",
    "    K_LU2 = incomplete_LU_decomp(K.copy())\n",
    "        \n",
    "    assert K_LU.shape == K_LU2.shape\n",
    "    for k in range(K_LU.shape[0]):\n",
    "        for i in range(K_LU.shape[1]):\n",
    "            assert abs(K_LU[k,i] - K_LU2[k,i]) < ep\n",
    "\n",
    "def H(beta,rho,loss_vect_list,opt_on):\n",
    "    start = time.time()\n",
    "    \n",
    "#     assert loss_type != 'hinge'\n",
    "\n",
    "    if opt_on=='b':\n",
    "        ret = 2*K + 2/(v*m)*np.sum(np.multiply(K[:,loss_vect_list],K[:,loss_vect_list]), axis=0)\n",
    "    elif opt_on == 'rho':\n",
    "        ret = 2/(v*m)\n",
    "\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'H:',end - start,'sec'\n",
    "    return ret\n",
    "\n",
    "def loss_der_der(t,rho):\n",
    "    if loss_type == 'hinge':\n",
    "        return 0\n",
    "    if loss_type == 'square-hinge':\n",
    "        if abs(rho - t) < delta:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2\n",
    "    raise Exception(loss_type,t,rho,delta)\n",
    "\n",
    "def loss_der_vect(old_grad,t,rho,opt_on):\n",
    "    if loss_type == 'hinge':\n",
    "        old_grad[ np.absolute(rho - t) <= delta ] = 0\n",
    "        old_grad[ rho - t > delta ] = -1\n",
    "        old_grad[ -rho + t > delta ] = 1\n",
    "        return old_grad\n",
    "    if loss_type == 'square-hinge':\n",
    "        old_grad[ np.absolute(rho - t) <= delta ] = 0\n",
    "        if opt_on=='b':\n",
    "            old_grad[ rho - t > delta ] = -2.0*(rho - t[rho - t > delta] - delta)\n",
    "            old_grad[ -rho + t > delta ] = 2.0*(-rho + t[-rho + t > delta] - delta)\n",
    "            return old_grad\n",
    "        if opt_on=='rho':\n",
    "            old_grad[ rho - t > delta ] = 2*(rho - t[rho - t > delta] - delta)\n",
    "            old_grad[ -rho + t > delta ] = -2*(-rho + t[-rho + t > delta] - delta)\n",
    "            return old_grad\n",
    "    raise Exception(old_grad,loss_type,t,rho,delta)\n",
    "        \n",
    "def kernel(x1,x2):\n",
    "    return math.exp(-1*math.pow(norm(x1-x2),2\n",
    "                               )/(2*math.pow(sigma, 2)))\n",
    "\n",
    "def kernel_vect(x_list,x2):\n",
    "    return np.exp(-1*np.power(norm(x_list-x2,axis=1),2 )/(2*math.pow(sigma, 2)))\n",
    "\n",
    "def loss_vect(t,rho):\n",
    "    if loss_type == 'hinge':\n",
    "        return np.max(0, np.absolute(rho - t) - delta )\n",
    "    if loss_type == 'square-hinge':\n",
    "        return np.power(np.maximum(np.zeros(t.shape), np.absolute(rho - t) - delta ),2)\n",
    "\n",
    "def loss_matrix_rho_vect(t,rho):\n",
    "    rho = np.matrix(rho).T\n",
    "    t = np.matrix(t)\n",
    "    if loss_type == 'hinge':\n",
    "        return np.maximum(np.zeros((t.shape[0],rho.shape[1])), np.absolute(t - rho) - delta )\n",
    "    if loss_type == 'square-hinge':\n",
    "        return np.power(np.maximum(np.zeros((t.shape[0],rho.shape[1])), np.absolute(t - rho) - delta ),2)\n",
    "\n",
    "def loss_matrix_t_matrix(t,rho):\n",
    "    if loss_type == 'hinge':\n",
    "        return np.maximum(np.zeros(t.shape), np.absolute(t - rho) - delta )\n",
    "    if loss_type == 'square-hinge':\n",
    "        return np.power(np.maximum(np.zeros(t.shape), np.absolute(t - rho) - delta ),2)\n",
    "    \n",
    "def obj_funct(beta,rho):\n",
    "    start = time.time()\n",
    "\n",
    "    obj = 1.0/2*np.dot(beta,np.dot(K,beta)) + 1.0 / (v*m) * np.sum(loss_vect(np.dot(K,beta),rho))\n",
    "    \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'obj_funct:',end - start,'sec'\n",
    "    return obj\n",
    "    \n",
    "def obj_funct_beta_matrix(beta,rho):\n",
    "    start = time.time()\n",
    "    \n",
    "    obj = 1.0/2*(norm(np.multiply(beta,np.dot(K,beta)),axis=0)) \\\n",
    "            + 1.0 / (v*m) * np.sum(loss_matrix_t_matrix(np.dot(K,beta),rho),axis=0)\n",
    "\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'obj_funct_beta_matrix:',end - start,'sec'\n",
    "    return obj\n",
    "\n",
    "def obj_funct_rho_vect(beta,rho):\n",
    "    start = time.time()\n",
    "\n",
    "    obj = 1.0/2*np.dot(beta,np.dot(K,beta)) + 1.0 / (v*m) * np.sum(loss_matrix_rho_vect(np.dot(K,beta),rho),axis=1)\n",
    "#     assert len(obj) == len(rho)\n",
    "    \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'obj_funct_rho_vect:',end - start,'sec'\n",
    "    return obj\n",
    "\n",
    "def f(x_test, beta,rho):\n",
    "    start = time.time()\n",
    "    \n",
    "    w = np.dot(beta,kernel_vect(x,x_test)) - rho\n",
    "            \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'f:',end - start,'sec'\n",
    "    return w\n",
    "\n",
    "def step(element,step_size,resid):\n",
    "    return element - (step_size * resid)\n",
    "\n",
    "def backtrack_step_size_rho(step_size,obj,resid,beta,rho):\n",
    "    start = time.time()\n",
    "    number_of_steps = 2\n",
    "    if step_size == ep**2:\n",
    "        step_size = initial_step_size\n",
    "    else:\n",
    "        step_size *= 2.0\n",
    "        \n",
    "    iter_count=0\n",
    "    \n",
    "    if obj > (obj_funct( beta, step(rho,step_size,resid)) ):\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'backtrack_step_size_rho:',end - start,'sec',iter_count,'times'\n",
    "        return step_size\n",
    "    \n",
    "    while True:\n",
    "        iter_count += 1\n",
    "        steps = step_size*np.logspace(-1,-number_of_steps,num = number_of_steps,base=2.0)\n",
    "        obj_many_steps = np.array(obj_funct_rho_vect( beta, step(rho,steps,resid))).ravel()\n",
    "\n",
    "        if np.where(obj - obj_many_steps >= 0)[0].shape[0] > 0:\n",
    "            end = time.time()\n",
    "            if end - start > timer_thresh:\n",
    "                print 'backtrack_step_size_rho:',end - start,'sec',iter_count,'times'\n",
    "            return steps[np.where(obj - obj_many_steps >= 0)[0][0]]\n",
    "                         \n",
    "        step_size = steps[-1]\n",
    "        if step_size < ep**2:\n",
    "#             print 'WARNING: step size not found'\n",
    "            step_size = ep**2\n",
    "            end = time.time()\n",
    "            if end - start > timer_thresh:\n",
    "                print 'backtrack_step_size_rho:',end - start,'sec',iter_count,'times'\n",
    "            return step_size\n",
    "        number_of_steps *= 2\n",
    "\n",
    "    assert False\n",
    "\n",
    "def backtrack_step_size_beta(step_size,obj,resid,beta,rho):\n",
    "    start = time.time()\n",
    "    number_of_steps = 2\n",
    "    if step_size == ep**2:\n",
    "        step_size = initial_step_size\n",
    "    else:\n",
    "        step_size *= 2.0\n",
    "\n",
    "    iter_count=0\n",
    "        \n",
    "    if obj > (obj_funct( step(beta,step_size,resid),rho) ):\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'backtrack_step_size_beta:',end - start,'sec',iter_count,'times'\n",
    "        return step_size\n",
    "\n",
    "    while True:\n",
    "        iter_count += 1\n",
    "        steps = step_size*np.logspace(-1,-number_of_steps,num = number_of_steps,base=2.0)\n",
    "        obj_many_steps = (obj_funct_beta_matrix(np.asarray(step(np.matrix(beta),\n",
    "                                                                np.matrix(steps).T,\n",
    "                                                                np.matrix(resid)).T\n",
    "                                                          ), \n",
    "                                                rho))\n",
    "        \n",
    "        if np.where(obj - obj_many_steps >= 0)[0].shape[0] > 0:\n",
    "            end = time.time()\n",
    "            if end - start > timer_thresh:\n",
    "                print 'backtrack_step_size_beta:',end - start,'sec',iter_count,'times'\n",
    "            return steps[np.where(obj - obj_many_steps >= 0)[0][0]]\n",
    "                         \n",
    "        step_size = steps[-1]\n",
    "        if step_size < ep**2:\n",
    "#             print 'WARNING: step size not found'\n",
    "            step_size = ep**2\n",
    "            end = time.time()\n",
    "            if end - start > timer_thresh:\n",
    "                print 'backtrack_step_size_beta:',end - start,'sec',iter_count,'times'\n",
    "            return step_size\n",
    "        number_of_steps *= 10\n",
    "\n",
    "    assert False\n",
    "\n",
    "def numer_grad(beta,rho,ep,direct=0,opt_on=''): # const\n",
    "    if opt_on == 'rho':\n",
    "        return (obj_funct(beta,rho+ep) \\\n",
    "               -obj_funct(beta,-rho*ep))/(2*ep)\n",
    "    return (obj_funct(beta+(ep*direct),rho) \\\n",
    "           -obj_funct(beta-(ep*direct),rho))/(2*ep)\n",
    "\n",
    "def grad_checker(beta,rho,ep,opt_on): # const\n",
    "    start = time.time()\n",
    "\n",
    "    if opt_on == 'rho':\n",
    "        return numer_grad(beta,rho,ep,opt_on=opt_on)\n",
    "\n",
    "    d=len(beta)\n",
    "    w=np.zeros(d)\n",
    "    for i in range(d):\n",
    "        direct=np.zeros(beta.shape)\n",
    "        direct[i] = 1\n",
    "        w[i]=(numer_grad(beta,rho,ep,direct=direct,opt_on=opt_on))\n",
    "        \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'grad_checker:',end - start,'sec'        \n",
    "    return w\n",
    "\n",
    "def get_resid(beta,rho,grad,loss_vect_list,opt_on):\n",
    "    start = time.time()\n",
    "    \n",
    "    if opt_on=='b':\n",
    "        resid = linalg.solve(H(beta,rho,loss_vect_list,opt_on),grad)\n",
    "    else:\n",
    "        resid = grad/H(beta,rho,loss_vect_list,opt_on)\n",
    "        \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'get_resid:',end - start,'sec'\n",
    "    return resid\n",
    "\n",
    "def grad_des_iterate(grad,beta,rho,step_size_beta,step_size_rho,obj_array,obj_grad_array,\n",
    "                     obj_grad_check_array,iterations,opt_on='b'):\n",
    "    start = time.time()\n",
    "    loss_vect_list = np.where(np.absolute(rho - np.dot(K,beta)) >= delta)[0]            \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'find sv:',end - start,'sec'\n",
    "\n",
    "    obj = obj_funct(beta,rho)\n",
    "#     print 'obj',obj\n",
    "    obj_array[iterations]=(obj)\n",
    "\n",
    "    if opt_on == 'b':\n",
    "        grad = 2.0*np.dot(K,beta) + 1.0/(v*m)*np.sum((K*loss_der_vect(grad,np.dot(K,beta),rho,opt_on)),axis=0)\n",
    "    elif opt_on == 'rho':\n",
    "        grad = 1/(v*m)*np.sum(loss_der_vect(grad,np.dot(K,beta),rho,opt_on))\n",
    "    \n",
    "    obj_grad_array[iterations]=norm(grad)\n",
    "#     obj_grad_check_array[iterations]=norm((grad-grad_checker(beta,rho,ep,opt_on)))\n",
    "\n",
    "    if obj < ep:\n",
    "        print 'Stopping crit: obj small',obj\n",
    "        return True,beta,rho,step_size_beta,step_size_rho,obj_array,obj_grad_array,obj_grad_check_array\n",
    "    if norm(grad) < ep:\n",
    "        print 'Stopping crit: norm(grad) small',norm(grad)\n",
    "        return True,beta,rho,step_size_beta,step_size_rho,obj_array,obj_grad_array,obj_grad_check_array\n",
    "\n",
    "    if loss_type == 'square-hinge' and method == 'Newton':\n",
    "        resid = get_resid(beta,rho,grad,loss_vect_list,opt_on)\n",
    "    else:\n",
    "        resid = (grad)\n",
    "\n",
    "    if opt_on == 'rho':\n",
    "        step_size_rho = backtrack_step_size_rho(step_size_rho,obj,resid,beta,rho)\n",
    "        rho = step(rho,step_size_rho,resid) # Update\n",
    "    else:\n",
    "        step_size_beta = backtrack_step_size_beta(step_size_beta,obj,resid,beta,rho)\n",
    "        beta = step(beta,step_size_beta,resid) # Update\n",
    "\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'grad_des_iterate:',end - start,'sec'\n",
    "\n",
    "    return False,beta,rho,step_size_beta,step_size_rho,obj_array,obj_grad_array,obj_grad_check_array\n",
    "\n",
    "def grad_des():\n",
    "    start = time.time()\n",
    "    obj_array = -1*np.ones(max_iter)\n",
    "    obj_grad_array = np.zeros((max_iter))\n",
    "    obj_grad_check_array = np.zeros(max_iter)\n",
    "\n",
    "    beta = zeros(m)\n",
    "    grad = zeros(beta.shape)\n",
    "    step_size_beta = initial_step_size\n",
    "    step_size_rho = initial_step_size\n",
    "    rho = initial_rho\n",
    "    iterations = 0\n",
    "    for i in range(max_iter):\n",
    "        converged,beta,rho,step_size_beta,step_size_rho,obj_array,obj_grad_array,obj_grad_check_array = \\\n",
    "                        grad_des_iterate(grad, beta,rho,step_size_beta,step_size_rho,obj_array,obj_grad_array,\n",
    "                                         obj_grad_check_array,iterations,opt_on='b')\n",
    "        \n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "        converged,beta,rho,step_size_beta,step_size_rho,obj_array,obj_grad_array,obj_grad_check_array = \\\n",
    "                        grad_des_iterate(grad, beta,rho,step_size_beta,step_size_rho,obj_array,obj_grad_array,\n",
    "                                         obj_grad_check_array,iterations,opt_on='rho')\n",
    "        \n",
    "        if converged:\n",
    "            break\n",
    "        \n",
    "        if i == max_iter-1:\n",
    "            print 'WARNING: Did not converge'\n",
    "            \n",
    "        iterations += 1\n",
    "\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'grad_des:',end - start,'sec'\n",
    "    return Run(obj_array,obj_grad_array,obj_grad_check_array,beta,rho,iterations)\n",
    "\n",
    "def get_data_points():\n",
    "    start = time.time()\n",
    "    points = random.random((points_count,2))*2*np.pi\n",
    "\n",
    "    x = np.zeros((points_count,3))\n",
    "    for p in range(points_count):\n",
    "        if points_std_from_surface > 0:\n",
    "            r = random.normal(loc=1,scale=points_std_from_surface)\n",
    "        else:\n",
    "            r = 1\n",
    "        z_cord = r * np.sin(points[p][1])\n",
    "\n",
    "        r_temp = r * np.cos(points[p][1])\n",
    "        y_cord = r_temp * np.sin(points[p][0])\n",
    "        x_cord = r_temp * np.cos(points[p][0])\n",
    "\n",
    "        x[p] = np.asarray([x_cord, y_cord, z_cord])\n",
    "            \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'get_data_points:',end - start,'sec'\n",
    "    return x\n",
    "\n",
    "def get_K():\n",
    "    start = time.time()\n",
    "\n",
    "    K=np.zeros((m,m))\n",
    "    for i in range(m):\n",
    "        K[i,:] = kernel_vect(x,x[i])\n",
    "        \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'get_K:',end - start,'sec'\n",
    "    return K\n",
    "\n",
    "def get_K_inv(K):\n",
    "    start = time.time()\n",
    "\n",
    "    K_inv = inv(K)\n",
    "    \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'get_K_inv:',end - start,'sec'\n",
    "    return K_inv\n",
    "\n",
    "def get_K_cond(K):\n",
    "    start = time.time()\n",
    "\n",
    "    K_cond = linalg.cond(K)\n",
    "    \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'get_K_cond:',end - start,'sec'\n",
    "    return K_cond\n",
    "\n",
    "def pre_comp_K():\n",
    "    start = time.time()\n",
    "\n",
    "    K = get_K()    \n",
    "    \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'pre_comp_K:',end - start,'sec'\n",
    "    return K #, K_inv\n",
    "\n",
    "x = get_data_points()        \n",
    "m = len(x)\n",
    "        \n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2])\n",
    "plt.show()\n",
    "\n",
    "K = get_K()    \n",
    "\n",
    "Run = namedtuple('Run', ['obj_array','obj_grad_array','obj_grad_check_array','beta','rho',\n",
    "                         'iterations'])\n",
    "\n",
    "Desc = {}\n",
    "counter=0\n",
    "for loss_type in ['square-hinge', 'hinge']:\n",
    "    for method in ['Newton', '']:\n",
    "        \n",
    "        print '-----------------------------------'\n",
    "        print 'loss_type',loss_type\n",
    "        print 'method',method        \n",
    "        Desc[counter] = grad_des()\n",
    "        print 'Desc iterations',Desc[counter].iterations\n",
    "        print 'Desc rho',Desc[counter].rho\n",
    "        print '-----------------------------------'\n",
    "        print \n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_steps = 25\n",
    "\n",
    "def pop_data_grid(beta,rho):\n",
    "    start = time.time()\n",
    "    data = np.zeros((grid_steps,grid_steps,grid_steps))\n",
    "\n",
    "    x0_range = np.linspace(-2, 2, grid_steps)\n",
    "    x1_range = np.linspace(-2, 2, grid_steps)\n",
    "    x2_range = np.linspace(-2, 2, grid_steps)\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'alloc mem:',end - start\n",
    "\n",
    "    for i in range(grid_steps):\n",
    "        for j in range(grid_steps):\n",
    "            for k in range(grid_steps):\n",
    "                data[i,j,k] = f(np.asarray([x0_range[i],\n",
    "                                x1_range[j],\n",
    "                                x2_range[k]]), beta,rho)\n",
    "                \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'pop_data_grid:',end - start,'sec'\n",
    "    return data\n",
    "\n",
    "def proc_data(beta,rho,data):\n",
    "    start = time.time()\n",
    "\n",
    "    print 'delta',delta\n",
    "    print 'np.abs(data - delta) < .1 -> ',(np.where(np.abs(data - delta) < .1)[0].shape)\n",
    "    print 'np.abs(data - delta) < .01 -> ',(np.where(np.abs(data - delta) < .01)[0].shape)\n",
    "    print 'np.abs(data - delta) < .001 -> ',(np.where(np.abs(data - delta) < .001)[0].shape)\n",
    "    print 'np.abs(data - delta) < .0001 -> ',(np.where(np.abs(data - delta) < .0001)[0].shape)\n",
    "    print 'data < delta -> ',(np.where(data < delta )[0].shape)\n",
    "    print 'data > delta -> ',(np.where(data > delta )[0].shape)\n",
    "    print 'data < 0 -> ',(np.where( data < 0)[0].shape)\n",
    "    print 'data == 0 -> ',(np.where( data == 0)[0].shape)\n",
    "    print 'data > 0 -> ',(np.where( data > 0)[0].shape)\n",
    "    print 'min -> ',(np.amin( data ))\n",
    "    print 'max -> ',(np.amax( data ))\n",
    "#     print 'data:',data\n",
    "    \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'proc_results:',end - start\n",
    "\n",
    "rho = Desc[0].rho\n",
    "beta = Desc[0].beta\n",
    "\n",
    "losses = []\n",
    "for i in range(m):\n",
    "    losses.append(f(x[i], beta, rho))\n",
    "\n",
    "data = pop_data_grid(beta,rho)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "from skimage import measure\n",
    "from skimage.draw import ellipsoid\n",
    "\n",
    "# Use marching cubes to obtain the surface mesh of these ellipsoids\n",
    "verts, faces = measure.marching_cubes(data, 0)\n",
    "\n",
    "# Display resulting triangular mesh using Matplotlib. This can also be done\n",
    "# with mayavi (see skimage.measure.marching_cubes docstring).\n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Fancy indexing: `verts[faces]` to generate a collection of triangles\n",
    "mesh = Poly3DCollection(verts[faces])\n",
    "ax.add_collection3d(mesh)\n",
    "\n",
    "ax.set_xlabel(\"x-axis\")\n",
    "ax.set_ylabel(\"y-axis\")\n",
    "ax.set_zlabel(\"z-axis\")\n",
    "\n",
    "ax.set_xlim(-0, 30)  \n",
    "ax.set_ylim(-0, 30)  \n",
    "ax.set_zlim(-0, 30)  \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.scatter(range(1,Newton_Desc.iterations+1),\n",
    "           Newton_Desc.obj_array[0:Newton_Desc.iterations],marker='^',\n",
    "           label='Non-Stochastic Newtons Method')\n",
    "ax.scatter(range(1,Steepest_Desc.iterations+1),\n",
    "           Steepest_Desc.obj_array[0:Steepest_Desc.iterations],marker='*',\n",
    "           label='Non-Stochastic Steepest Descent')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Objective Function over iterations')\n",
    "plt.ylabel('F (w)')\n",
    "plt.xlabel('Iteration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.scatter(range(1,(Newton_Desc.iterations)+1),\n",
    "           Newton_Desc.obj_grad_array[0:Newton_Desc.iterations],\n",
    "           marker='^',\n",
    "           label='Non-Stochastic Newtons Method')\n",
    "\n",
    "ax.scatter(range(1,(Steepest_Desc.iterations)+1),\n",
    "           Steepest_Desc.obj_grad_array[0:Steepest_Desc.iterations],\n",
    "           marker='*', \n",
    "           label='Non-Stochastic Steepest Descent')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Gradient Norm over iterations')\n",
    "plt.ylabel('norm(d/dw F (w))')\n",
    "plt.xlabel('Iteration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.scatter(range(1,(Steepest_Desc.iterations)+1),\n",
    "           Steepest_Desc.obj_grad_check_array[0:Steepest_Desc.iterations],\n",
    "           marker='*',\n",
    "           label='Non-Stochastic Steepest Descent')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Gradient Norm and Approx. Gradient Norm Difference \\n over iterations')\n",
    "plt.ylabel('Difference')\n",
    "plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
