{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal Slab SVM\n",
    "Let $K \\in R^{m \\times m}$ and $K_{ij} = \\texttt{kernel}(x_i,x_j)$ and $K_{i}$ the $i^{th}$ column of $K$\n",
    "\n",
    "Then Primal Minimization Objective:\n",
    "$$\\min_{\\beta \\in R^m,\\rho \\in R} \\beta^T K \\beta + \\frac{1}{\\nu m} \\sum_i \\texttt{loss}(K_i^T \\beta, \\rho)$$\n",
    "\n",
    "Let $F$ be the objective function.\n",
    "$$F(\\beta,\\rho) = \\beta^T K \\beta + \\frac{1}{\\nu m} \\sum_i \\texttt{loss}(K_i^T \\beta, \\rho)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients:\n",
    "$$\\vec\\nabla_\\beta F(\\beta,\\rho) = 2K\\beta + \\frac {1}{\\nu m} \\sum_i K_i \\circ \\frac{d}{d\\beta}\\texttt{loss}(K_i^T \\beta, \\rho)$$\n",
    "$$\\nabla_\\rho F(\\beta,\\rho) = \\frac {1}{\\nu m} \\sum_i \\frac{d}{d\\rho}\\texttt{loss}(K_i^T \\beta, \\rho)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hessians:\n",
    "$$H_\\beta = 2K + \\frac {1}{\\nu m} \\sum_i \\left( K_i \\circ K_i \\right) \\circ \\frac{d^2}{(d\\beta)^2}\\texttt{loss}(K_i^T \\beta, \\rho)$$\n",
    "$$H_\\rho = \\frac {1}{\\nu m} \\sum_i \\frac{d^2}{(d\\rho)^2}\\texttt{loss}(K_i^T \\beta, \\rho)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider losses:\n",
    "$$\\texttt{loss}_{hinge}(t,\\rho) = \\max(~0,~ |~\\rho - t~| - \\delta ~)$$\n",
    "$$\\texttt{loss}_{square-hinge}(t,\\rho) = \\max(~0,~ |~\\rho - t~| - \\delta ~)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Gradients:\n",
    "$$\\frac{d}{dt}\\texttt{loss}_{hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "-1, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "1, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d}{dt}\\texttt{loss}_{square-hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "-2\\left(\\rho-t-\\delta\\right), & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "2\\left(-\\rho+t-\\delta\\right), & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{d\\rho}\\texttt{loss}_{hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "1, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "1, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d}{d\\rho}\\texttt{loss}_{square-hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "2\\left(\\rho-t-\\delta\\right), & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "-2\\left(-\\rho+t-\\delta\\right), & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Hessians:\n",
    "$$\\frac{d^2}{(dt)^2}\\texttt{loss}_{hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "0, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "0, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d^2}{(dt)^2}\\texttt{loss}_{square-hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "2, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "2, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d^2}{(d\\rho)^2}\\texttt{loss}_{hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "0, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "0, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n",
    "\n",
    "$$\\frac{d^2}{(d\\rho)^2}\\texttt{loss}_{square-hinge}(t,\\rho) = \\begin{cases} 0, & \\mbox{if } |~\\rho - t~| \\lt \\delta \\\\ \n",
    "2, & \\mbox{if } ~\\rho - t~ \\gt \\delta  \\\\\n",
    "2, & \\mbox{if } ~-\\rho + t~ \\gt \\delta  \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:\n",
    "$$ \\langle \\Phi(x), w\\rangle = \\sum_k \\beta_k k(x_k, x) $$\n",
    "Surface:\n",
    "$$ \\langle \\Phi(x), w\\rangle -\\rho = \\sum_k \\beta_k k(x_k, x) -\\rho $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg, random, ones, zeros, matrix, eye, dot\n",
    "from numpy.linalg import norm,cholesky,inv\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import mosek\n",
    "import math\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time \n",
    "from collections import namedtuple\n",
    "\n",
    "v=.00001\n",
    "delta = 0.0\n",
    "sigma = 100\n",
    "initial_rho = 1\n",
    "max_iter = 1000\n",
    "initial_step_size = .1\n",
    "timer_thresh = .1\n",
    "ep = .000001\n",
    "points_count = 501\n",
    "points_std_from_surface = 0\n",
    "\n",
    "\n",
    "# class Primal_Opt:\n",
    "class Slab_SVM:\n",
    "    def pivot(A,k,n):\n",
    "        y = np.amax(np.absolute(A[k:n+1, k:n+1]),axis=1)\n",
    "        i = np.argmax(np.absolute(A[k:n+1, k:n+1]),axis=1)\n",
    "        piv = np.amax(y)\n",
    "        jpiv = np.argmax(y)\n",
    "        ipiv = i[jpiv]\n",
    "        jpiv = jpiv+k-1;\n",
    "        ipiv = ipiv + k-1;\n",
    "        Pk=eye(n)\n",
    "        Pk[ipiv,ipiv]=0\n",
    "        Pk[k,k]=0\n",
    "        Pk[k,ipiv]=1\n",
    "        Pk[ipiv,k]=1\n",
    "        Qk=eye(n)\n",
    "        Qk[jpiv,jpiv]=0\n",
    "        Qk[k,k]=0\n",
    "        Qk[k,jpiv]=1\n",
    "        Qk[jpiv,k]=1\n",
    "        return Pk,Qk\n",
    "\n",
    "    def incomplete_LU_decomp(A):\n",
    "        start = time.time()\n",
    "\n",
    "        assert A.shape[0] == A.shape[1]\n",
    "        n = A.shape[0]\n",
    "        for k in range(n-1):\n",
    "            Pk,Qk = pivot(A,k,n)\n",
    "            A=dot(dot(Pk,A),Qk)\n",
    "            print A\n",
    "            for i in range(k+1,n):\n",
    "                if A[i,k] != 0:\n",
    "                    if A[k,k] == 0:\n",
    "                        return 'Error: Null Pivot'\n",
    "                    A[i,k] = A[i,k]/A[k,k]\n",
    "                    for j in range(k+1,n):\n",
    "                        if A[i,j] != 0:\n",
    "                            A[i,j] = A[i,j] - (A[i,k]/A[k,j])\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'incomplete_LU_decomp:',end - start,'sec'\n",
    "        return A\n",
    "\n",
    "    import scipy\n",
    "    def get_K_LU():\n",
    "        K_LU = scipy.linalg.cholesky(K, lower=True)\n",
    "        K_LU = cholesky(K)\n",
    "        K_LU2 = incomplete_LU_decomp(K.copy())\n",
    "\n",
    "        assert K_LU.shape == K_LU2.shape\n",
    "        for k in range(K_LU.shape[0]):\n",
    "            for i in range(K_LU.shape[1]):\n",
    "                assert abs(K_LU[k,i] - K_LU2[k,i]) < ep\n",
    "\n",
    "    def H(self,beta,rho,loss_vect_list,opt_on):\n",
    "        start = time.time()\n",
    "\n",
    "    #     assert g_loss_type != 'hinge'\n",
    "\n",
    "        if opt_on=='b':\n",
    "            ret = 2*self.K + 2/(v*self.m)*np.sum(np.multiply(self.K[:,loss_vect_list],self.K[:,loss_vect_list]), \n",
    "                                                 axis=0)\n",
    "        elif opt_on == 'rho':\n",
    "            ret = 2/(v*self.m)\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'H:',end - start,'sec'\n",
    "        return ret\n",
    "\n",
    "    def loss_der_der(self,t,rho):\n",
    "        if g_loss_type == 'hinge':\n",
    "            return 0\n",
    "        if g_loss_type == 'square-hinge':\n",
    "            if abs(rho - t) < delta:\n",
    "                return 0\n",
    "            else:\n",
    "                return 2\n",
    "        raise Exception(g_loss_type,t,rho,delta)\n",
    "\n",
    "    def loss_der_vect(self,grad,t,rho,opt_on):\n",
    "#         print 'grad',self.grad.shape\n",
    "        if g_loss_type == 'hinge':\n",
    "            grad[ np.absolute(rho - t) <= delta ] = 0\n",
    "            grad[ rho - t > delta ] = -1\n",
    "            grad[ -rho + t > delta ] = 1\n",
    "            return grad\n",
    "        if g_loss_type == 'square-hinge':\n",
    "            grad[ np.absolute(rho - t) <= delta ] = 0\n",
    "            if opt_on=='b':\n",
    "                grad[ rho - t > delta ] = -2.0*(rho - t[rho - t > delta] - delta)\n",
    "                grad[ -rho + t > delta ] = 2.0*(-rho + t[-rho + t > delta] - delta)\n",
    "                return grad\n",
    "            if opt_on=='rho':\n",
    "                grad[ rho - t > delta ] = 2*(rho - t[rho - t > delta] - delta)\n",
    "                grad[ -rho + t > delta ] = -2*(-rho + t[-rho + t > delta] - delta)\n",
    "                return grad\n",
    "        raise Exception(grad,g_loss_type,t,rho,delta)\n",
    "\n",
    "    def kernel(self,x1,x2):\n",
    "        return math.exp(-1*math.pow(norm(x1-x2),2\n",
    "                                   )/(2*math.pow(sigma, 2)))\n",
    "\n",
    "    def kernel_vect(self,x_list,x2):\n",
    "        return np.exp(-1*np.power(norm(x_list-x2,axis=1),2 )/(2*math.pow(sigma, 2)))\n",
    "\n",
    "    def loss_vect(self,t,rho):\n",
    "        if g_loss_type == 'hinge':\n",
    "            return np.maximum(np.zeros(t.shape), np.absolute(rho - t) - delta )\n",
    "        if g_loss_type == 'square-hinge':\n",
    "            return np.power(np.maximum(np.zeros(t.shape), np.absolute(rho - t) - delta ),2)\n",
    "\n",
    "    def loss_matrix_rho_vect(self,t,rho):\n",
    "        rho = np.matrix(rho).T\n",
    "        t = np.matrix(t)\n",
    "        if g_loss_type == 'hinge':\n",
    "            return np.maximum(np.zeros((t.shape[0],rho.shape[1])), np.absolute(t - rho) - delta )\n",
    "        if g_loss_type == 'square-hinge':\n",
    "            return np.power(np.maximum(np.zeros((t.shape[0],rho.shape[1])), np.absolute(t - rho) - delta ),2)\n",
    "\n",
    "    def loss_matrix_t_matrix(self,t,rho):\n",
    "        if g_loss_type == 'hinge':\n",
    "            return np.maximum(np.zeros(t.shape), np.absolute(t - rho) - delta )\n",
    "        if g_loss_type == 'square-hinge':\n",
    "            return np.power(np.maximum(np.zeros(t.shape), np.absolute(t - rho) - delta ),2)\n",
    "\n",
    "    def obj_funct(self,beta,rho):\n",
    "        start = time.time()\n",
    "\n",
    "        obj = 1.0/2*np.dot(beta,np.dot(self.K,beta)) + 1.0 / (v*self.m) * np.sum(\n",
    "                                                                            self.loss_vect(np.dot(self.K,beta),\n",
    "                                                                                           rho))\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'obj_funct:',end - start,'sec'\n",
    "        return obj\n",
    "\n",
    "    def obj_funct_beta_matrix(self,beta,rho):\n",
    "        start = time.time()\n",
    "\n",
    "        obj = 1.0/2*(norm(np.multiply(beta,np.dot(self.K,beta)),axis=0)) \\\n",
    "                + 1.0 / (v*self.m) * np.sum(self.loss_matrix_t_matrix(np.dot(self.K,beta),rho),axis=0)\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'obj_funct_beta_matrix:',end - start,'sec'\n",
    "        return obj\n",
    "\n",
    "    def obj_funct_rho_vect(self,beta,rho):\n",
    "        start = time.time()\n",
    "\n",
    "        obj = 1.0/2*np.dot(beta,np.dot(self.K,beta)) + 1.0 / (v*self.m) * np.sum(\n",
    "                                                                    self.loss_matrix_rho_vect(np.dot(self.K,beta),\n",
    "                                                                                                 rho),axis=1)\n",
    "    #     assert len(obj) == len(rho)\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'obj_funct_rho_vect:',end - start,'sec'\n",
    "        return obj\n",
    "\n",
    "    def f(self,x_test, beta,rho):\n",
    "        start = time.time()\n",
    "\n",
    "        w = np.dot(beta,self.kernel_vect(x,x_test)) - rho\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'f:',end - start,'sec'\n",
    "        return w\n",
    "\n",
    "    def step(self,element,step_size,resid):\n",
    "        return element - (step_size * resid)\n",
    "\n",
    "    def backtrack_step_size_rho(self,step_size,obj,resid,beta,rho):\n",
    "        start = time.time()\n",
    "        number_of_steps = 2\n",
    "        if step_size == ep**2:\n",
    "            step_size = initial_step_size\n",
    "        else:\n",
    "            step_size *= 2.0\n",
    "\n",
    "        iter_count=0\n",
    "\n",
    "        if obj > (self.obj_funct( beta, self.step(rho,step_size,resid)) ):\n",
    "            end = time.time()\n",
    "            if end - start > timer_thresh:\n",
    "                print 'backtrack_step_size_rho:',end - start,'sec',iter_count,'times'\n",
    "            return step_size\n",
    "\n",
    "        while True:\n",
    "            iter_count += 1\n",
    "            steps = step_size*np.logspace(-1,-number_of_steps,num = number_of_steps,base=2.0)\n",
    "            obj_many_steps = np.array(self.obj_funct_rho_vect( beta, self.step(rho,steps,resid))).ravel()\n",
    "\n",
    "            if np.where(obj - obj_many_steps >= 0)[0].shape[0] > 0:\n",
    "                end = time.time()\n",
    "                if end - start > timer_thresh:\n",
    "                    print 'backtrack_step_size_rho:',end - start,'sec',iter_count,'times'\n",
    "                return steps[np.where(obj - obj_many_steps >= 0)[0][0]]\n",
    "\n",
    "            step_size = steps[-1]\n",
    "            if step_size < ep**2:\n",
    "    #             print 'WARNING: step size not found'\n",
    "                step_size = ep**2\n",
    "                end = time.time()\n",
    "                if end - start > timer_thresh:\n",
    "                    print 'backtrack_step_size_rho:',end - start,'sec',iter_count,'times'\n",
    "                return step_size\n",
    "            number_of_steps *= 2\n",
    "\n",
    "        assert False\n",
    "\n",
    "    def backtrack_step_size_beta(self,step_size,obj,resid,beta,rho):\n",
    "        start = time.time()\n",
    "        number_of_steps = 2\n",
    "        if step_size == ep**2:\n",
    "            step_size = initial_step_size\n",
    "        else:\n",
    "            step_size *= 2.0\n",
    "\n",
    "        iter_count=0\n",
    "\n",
    "        if obj > (self.obj_funct( self.step(beta,step_size,resid),rho) ):\n",
    "            end = time.time()\n",
    "            if end - start > timer_thresh:\n",
    "                print 'backtrack_step_size_beta:',end - start,'sec',iter_count,'times'\n",
    "            return step_size\n",
    "\n",
    "        while True:\n",
    "            iter_count += 1\n",
    "            steps = step_size*np.logspace(-1,-number_of_steps,num = number_of_steps,base=2.0)\n",
    "            obj_many_steps = (self.obj_funct_beta_matrix(np.asarray(self.step(np.matrix(beta),\n",
    "                                                                    np.matrix(steps).T,\n",
    "                                                                    np.matrix(resid)).T\n",
    "                                                              ), \n",
    "                                                    rho))\n",
    "\n",
    "            if np.where(obj - obj_many_steps >= 0)[0].shape[0] > 0:\n",
    "                end = time.time()\n",
    "                if end - start > timer_thresh:\n",
    "                    print 'backtrack_step_size_beta:',end - start,'sec',iter_count,'times'\n",
    "                return steps[np.where(obj - obj_many_steps >= 0)[0][0]]\n",
    "\n",
    "            step_size = steps[-1]\n",
    "            if step_size < ep**2:\n",
    "    #             print 'WARNING: step size not found'\n",
    "                step_size = ep**2\n",
    "                end = time.time()\n",
    "                if end - start > timer_thresh:\n",
    "                    print 'backtrack_step_size_beta:',end - start,'sec',iter_count,'times'\n",
    "                return step_size\n",
    "            number_of_steps *= 10\n",
    "\n",
    "        assert False\n",
    "\n",
    "    def numer_grad(self,beta,rho,ep,direct=0,opt_on=''): # const\n",
    "        if opt_on == 'rho':\n",
    "            return (obj_funct(beta,rho+ep) \\\n",
    "                   -obj_funct(beta,-rho*ep))/(2*ep)\n",
    "        return (obj_funct(beta+(ep*direct),rho) \\\n",
    "               -obj_funct(beta-(ep*direct),rho))/(2*ep)\n",
    "\n",
    "    def grad_checker(self,beta,rho,ep,opt_on): # const\n",
    "        start = time.time()\n",
    "\n",
    "        if opt_on == 'rho':\n",
    "            return numer_grad(beta,rho,ep,opt_on=opt_on)\n",
    "\n",
    "        d=len(beta)\n",
    "        w=np.zeros(d)\n",
    "        for i in range(d):\n",
    "            direct=np.zeros(beta.shape)\n",
    "            direct[i] = 1\n",
    "            w[i]=(numer_grad(beta,rho,ep,direct=direct,opt_on=opt_on))\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'grad_checker:',end - start,'sec'        \n",
    "        return w\n",
    "\n",
    "    def get_resid(self,beta,rho,grad,loss_vect_list,opt_on):\n",
    "        start = time.time()\n",
    "\n",
    "        if opt_on=='b':\n",
    "            resid = linalg.solve(self.H(beta,rho,loss_vect_list,opt_on),grad)\n",
    "        else:\n",
    "            resid = grad/self.H(beta,rho,loss_vect_list,opt_on)\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'get_resid:',end - start,'sec'\n",
    "        return resid\n",
    "\n",
    "    def grad_des_iterate(self,iterations,opt_on='b'):\n",
    "        start = time.time()\n",
    "        loss_vect_list = np.where(np.absolute(self.rho - np.dot(self.K,self.beta)) >= delta)[0]            \n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'find sv:',end - start,'sec'\n",
    "\n",
    "        obj = self.obj_funct(self.beta,self.rho)\n",
    "    #     print 'obj',obj\n",
    "        self.obj_array[iterations]=(obj)\n",
    "\n",
    "        if opt_on == 'b':\n",
    "            self.grad = 2.0*np.dot(self.K,self.beta) + 1.0/(v*self.m)*np.sum(\n",
    "                                                                (self.K * self.loss_der_vect(\n",
    "                                                                                             self.grad_buffer,\n",
    "                                                                                             np.dot(self.K,self.beta),\n",
    "                                                                                             self.rho,\n",
    "                                                                                             opt_on)),\n",
    "                                                                             axis=0)\n",
    "        elif opt_on == 'rho':\n",
    "            self.grad = 1/(v*self.m)*np.sum(self.loss_der_vect(\n",
    "                    self.grad_buffer,\n",
    "                    np.dot(self.K,self.beta),self.rho,              opt_on))\n",
    "\n",
    "        self.obj_grad_array[iterations]=norm(self.grad)\n",
    "    #     obj_grad_check_array[iterations]=norm((grad-grad_checker(beta,rho,ep,opt_on)))\n",
    "\n",
    "        if obj < ep:\n",
    "            print 'Stopping crit: obj small',obj\n",
    "            return True\n",
    "            return True,beta,rho,step_size_beta,step_size_rho,obj_array,obj_grad_array,obj_grad_check_array\n",
    "        if norm(self.grad) < ep:\n",
    "            print 'Stopping crit: norm(grad) small',norm(self.grad)\n",
    "            return True\n",
    "            return True,beta,rho,step_size_beta,step_size_rho,obj_array,obj_grad_array,obj_grad_check_array\n",
    "\n",
    "        if g_loss_type == 'square-hinge' and g_method == 'Newton':\n",
    "            resid = self.get_resid(self.beta,self.rho,self.grad,loss_vect_list,opt_on)\n",
    "        else:\n",
    "            resid = self.grad\n",
    "\n",
    "        if opt_on == 'rho':\n",
    "            self.step_size_rho = self.backtrack_step_size_rho(self.step_size_rho,obj,resid,self.beta,self.rho)\n",
    "            self.rho = self.step(self.rho,self.step_size_rho,resid) # Update\n",
    "        else:\n",
    "            self.step_size_beta = self.backtrack_step_size_beta(self.step_size_beta,obj,resid,self.beta,self.rho)\n",
    "            self.beta = self.step(self.beta,self.step_size_beta,resid) # Update\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'grad_des_iterate:',end - start,'sec'\n",
    "\n",
    "        return False\n",
    "        return False,beta,rho,step_size_beta,step_size_rho,obj_array,obj_grad_array,obj_grad_check_array\n",
    "\n",
    "    def grad_des(self, prev_run=None):\n",
    "        start = time.time()\n",
    "\n",
    "        self.pop_K()\n",
    "\n",
    "        self.obj_array = -1*np.ones(max_iter)\n",
    "        self.obj_grad_array = np.zeros((max_iter))\n",
    "        self.obj_grad_check_array = np.zeros(max_iter)\n",
    "\n",
    "        if prev_run == None:\n",
    "            self.beta = zeros(self.m)\n",
    "            self.rho = initial_rho\n",
    "        else:\n",
    "            self.beta = np.append(prev_run.beta,zeros(self.m-len(prev_run.beta)))\n",
    "            self.rho = prev_run.rho\n",
    "\n",
    "        self.grad_buffer = zeros(self.beta.shape)\n",
    "        self.step_size_beta = initial_step_size\n",
    "        self.step_size_rho = initial_step_size\n",
    "        iterations = 0\n",
    "        for i in range(max_iter):\n",
    "            \n",
    "            converged = self.grad_des_iterate(iterations,opt_on='b')\n",
    "            if converged:\n",
    "                break\n",
    "\n",
    "            converged = self.grad_des_iterate(iterations,opt_on='rho')\n",
    "            if converged:\n",
    "                break\n",
    "\n",
    "            if i == max_iter-1:\n",
    "                print 'WARNING: Did not converge'\n",
    "\n",
    "            iterations += 1\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'grad_des:',end - start,'sec'\n",
    "        return Run(self.obj_array,self.obj_grad_array,self.obj_grad_check_array,self.beta,self.rho,iterations)\n",
    "\n",
    "    def pop_K(self):\n",
    "        start = time.time()\n",
    "\n",
    "        self.K=np.zeros((self.m,self.m))\n",
    "        for i in range(self.m):\n",
    "            self.K[i,:] = self.kernel_vect(self.x_data,self.x_data[i])\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'get_K:',end - start,'sec'\n",
    "\n",
    "    def get_K_inv(K):\n",
    "        start = time.time()\n",
    "\n",
    "        K_inv = inv(K)\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'get_K_inv:',end - start,'sec'\n",
    "        return K_inv\n",
    "\n",
    "    def get_K_cond(K):\n",
    "        start = time.time()\n",
    "\n",
    "        K_cond = linalg.cond(K)\n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'get_K_cond:',end - start,'sec'\n",
    "        return K_cond\n",
    "\n",
    "    def pre_comp_K():\n",
    "        start = time.time()\n",
    "\n",
    "        K = get_K()    \n",
    "\n",
    "        end = time.time()\n",
    "        if end - start > timer_thresh:\n",
    "            print 'pre_comp_K:',end - start,'sec'\n",
    "        return K #, K_inv\n",
    "\n",
    "    def __init__(self, x_data):\n",
    "        self.x_data = x_data\n",
    "        self.m = len(self.x_data)\n",
    "\n",
    "    def run(self):\n",
    "        if len(self.x_data) <= 500:\n",
    "            return self.grad_des()\n",
    "        this_run = Slab_SVM(self.x_data[random.choice(len(self.x_data),500,replace=False)]).run()\n",
    "        return self.grad_des(this_run)\n",
    "\n",
    "def get_data_points():\n",
    "    start = time.time()\n",
    "    points = random.random((points_count,2))*2*np.pi\n",
    "\n",
    "    x = np.zeros((points_count,3))\n",
    "    for p in range(points_count):\n",
    "        if points_std_from_surface > 0:\n",
    "            r = random.normal(loc=1,scale=points_std_from_surface)\n",
    "        else:\n",
    "            r = 1\n",
    "        z_cord = r * np.sin(points[p][1])\n",
    "\n",
    "        r_temp = r * np.cos(points[p][1])\n",
    "        y_cord = r_temp * np.sin(points[p][0])\n",
    "        x_cord = r_temp * np.cos(points[p][0])\n",
    "\n",
    "        x[p] = np.asarray([x_cord, y_cord, z_cord])\n",
    "\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'get_data_points:',end - start,'sec'\n",
    "    return x\n",
    "    \n",
    "g_x = get_data_points()        \n",
    "\n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(g_x[:,0],g_x[:,1],g_x[:,2])\n",
    "plt.show()\n",
    "\n",
    "Run = namedtuple('Run', ['obj_array','obj_grad_array','obj_grad_check_array','beta','rho',\n",
    "                         'iterations'])\n",
    "\n",
    "g_Desc = {}\n",
    "g_counter=0\n",
    "for g_loss_type in ['square-hinge', 'hinge']:\n",
    "    for g_method in ['Newton', '']:\n",
    "\n",
    "        print '-----------------------------------'\n",
    "        print 'loss_type',g_loss_type\n",
    "        print 'method',g_method        \n",
    "        g_Desc[g_counter] = Slab_SVM(g_x).run()\n",
    "        print 'Desc iterations',g_Desc[g_counter].iterations\n",
    "        print 'Desc rho',g_Desc[g_counter].rho\n",
    "        print '-----------------------------------'\n",
    "        print \n",
    "\n",
    "        g_counter += 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_steps = 25\n",
    "\n",
    "def pop_data_grid(beta,rho):\n",
    "    start = time.time()\n",
    "    data = np.zeros((grid_steps,grid_steps,grid_steps))\n",
    "\n",
    "    x0_range = np.linspace(-2, 2, grid_steps)\n",
    "    x1_range = np.linspace(-2, 2, grid_steps)\n",
    "    x2_range = np.linspace(-2, 2, grid_steps)\n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'alloc mem:',end - start\n",
    "\n",
    "    for i in range(grid_steps):\n",
    "        for j in range(grid_steps):\n",
    "            for k in range(grid_steps):\n",
    "                data[i,j,k] = f(np.asarray([x0_range[i],\n",
    "                                x1_range[j],\n",
    "                                x2_range[k]]), beta,rho)\n",
    "                \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'pop_data_grid:',end - start,'sec'\n",
    "    return data\n",
    "\n",
    "def proc_data(beta,rho,data):\n",
    "    start = time.time()\n",
    "\n",
    "    print 'delta',delta\n",
    "    print 'np.abs(data - delta) < .1 -> ',(np.where(np.abs(data - delta) < .1)[0].shape)\n",
    "    print 'np.abs(data - delta) < .01 -> ',(np.where(np.abs(data - delta) < .01)[0].shape)\n",
    "    print 'np.abs(data - delta) < .001 -> ',(np.where(np.abs(data - delta) < .001)[0].shape)\n",
    "    print 'np.abs(data - delta) < .0001 -> ',(np.where(np.abs(data - delta) < .0001)[0].shape)\n",
    "    print 'data < delta -> ',(np.where(data < delta )[0].shape)\n",
    "    print 'data > delta -> ',(np.where(data > delta )[0].shape)\n",
    "    print 'data < 0 -> ',(np.where( data < 0)[0].shape)\n",
    "    print 'data == 0 -> ',(np.where( data == 0)[0].shape)\n",
    "    print 'data > 0 -> ',(np.where( data > 0)[0].shape)\n",
    "    print 'min -> ',(np.amin( data ))\n",
    "    print 'max -> ',(np.amax( data ))\n",
    "#     print 'data:',data\n",
    "    \n",
    "    end = time.time()\n",
    "    if end - start > timer_thresh:\n",
    "        print 'proc_results:',end - start\n",
    "\n",
    "rho = Desc[0].rho\n",
    "beta = Desc[0].beta\n",
    "\n",
    "losses = []\n",
    "for i in range(m):\n",
    "    losses.append(f(x[i], beta, rho))\n",
    "\n",
    "data = pop_data_grid(beta,rho)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "from skimage import measure\n",
    "from skimage.draw import ellipsoid\n",
    "\n",
    "# Use marching cubes to obtain the surface mesh of these ellipsoids\n",
    "verts, faces = measure.marching_cubes(data, 0)\n",
    "\n",
    "# Display resulting triangular mesh using Matplotlib. This can also be done\n",
    "# with mayavi (see skimage.measure.marching_cubes docstring).\n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Fancy indexing: `verts[faces]` to generate a collection of triangles\n",
    "mesh = Poly3DCollection(verts[faces])\n",
    "ax.add_collection3d(mesh)\n",
    "\n",
    "ax.set_xlabel(\"x-axis\")\n",
    "ax.set_ylabel(\"y-axis\")\n",
    "ax.set_zlabel(\"z-axis\")\n",
    "\n",
    "ax.set_xlim(-0, 30)  \n",
    "ax.set_ylim(-0, 30)  \n",
    "ax.set_zlim(-0, 30)  \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.scatter(range(1,Newton_Desc.iterations+1),\n",
    "           Newton_Desc.obj_array[0:Newton_Desc.iterations],marker='^',\n",
    "           label='Non-Stochastic Newtons Method')\n",
    "ax.scatter(range(1,Steepest_Desc.iterations+1),\n",
    "           Steepest_Desc.obj_array[0:Steepest_Desc.iterations],marker='*',\n",
    "           label='Non-Stochastic Steepest Descent')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Objective Function over iterations')\n",
    "plt.ylabel('F (w)')\n",
    "plt.xlabel('Iteration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.scatter(range(1,(Newton_Desc.iterations)+1),\n",
    "           Newton_Desc.obj_grad_array[0:Newton_Desc.iterations],\n",
    "           marker='^',\n",
    "           label='Non-Stochastic Newtons Method')\n",
    "\n",
    "ax.scatter(range(1,(Steepest_Desc.iterations)+1),\n",
    "           Steepest_Desc.obj_grad_array[0:Steepest_Desc.iterations],\n",
    "           marker='*', \n",
    "           label='Non-Stochastic Steepest Descent')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Gradient Norm over iterations')\n",
    "plt.ylabel('norm(d/dw F (w))')\n",
    "plt.xlabel('Iteration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "ax.scatter(range(1,(Steepest_Desc.iterations)+1),\n",
    "           Steepest_Desc.obj_grad_check_array[0:Steepest_Desc.iterations],\n",
    "           marker='*',\n",
    "           label='Non-Stochastic Steepest Descent')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Gradient Norm and Approx. Gradient Norm Difference \\n over iterations')\n",
    "plt.ylabel('Difference')\n",
    "plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
