{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement K-means and K-means++\n",
    "We see that the scikit-learn version centers the data to get more accurate distance computations, so we do so also.\n",
    "\n",
    "https://github.com/scikit-learn/scikit-learn/blob/c957249/sklearn/cluster/k_means_.py#L295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy.linalg import norm \n",
    "import random\n",
    "import math\n",
    "import time \n",
    "\n",
    "\n",
    "def get_buckets_of_samples(p,centroids):\n",
    "    C=[] # to hold samples near each centroid\n",
    "    for i in range(len(centroids)):\n",
    "        C.append([])\n",
    "\n",
    "    # for each sample\n",
    "    for i in range(len(p)):\n",
    "\n",
    "        # find nearest centroid\n",
    "        nearest_centroid = 0\n",
    "        for j in range(1,len(centroids)):\n",
    "            if norm(p[i]-centroids[j]) < \\\n",
    "                        norm(p[i]-centroids[nearest_centroid]):\n",
    "                nearest_centroid = j\n",
    "\n",
    "        C[nearest_centroid].append(i) # add sample's index to cluster\n",
    "    assert len(C) > 0\n",
    "    return C\n",
    "\n",
    "\n",
    "def mykmeans(p,k,iter,centroids=None):\n",
    "    y = np.zeros(len(p))\n",
    "    if centroids == None:\n",
    "        centroids = train_test_split(p.copy(), y, train_size=k, \n",
    "                                     random_state=10051999)[0]\n",
    "    tol = 0.000001\n",
    "    movement = tol + 1\n",
    "    counter = 0\n",
    "    err = 0\n",
    "    while movement > tol and counter < iter:\n",
    "        counter += 1\n",
    "        \n",
    "        # assign sample to cluster\n",
    "        C = get_buckets_of_samples(p,centroids)\n",
    "        assert len(C) > 0\n",
    "    \n",
    "        movement = 0.0\n",
    "        for i in range(k):\n",
    "            if len(C[i]) > 0:\n",
    "                cluster_sum = np.zeros(p[0].shape)\n",
    "                for j in range(len(C[i])):\n",
    "                    cluster_sum += p[C[i][j]]\n",
    "                movement += norm(centroids[i] - (cluster_sum / len(C[i])))\n",
    "                centroids[i] = cluster_sum / len(C[i])\n",
    "            else: # orphan cluster with no samples\n",
    "                centroids[i] = p[random.randint(0, len(p)-1)]\n",
    "            \n",
    "        err = 0\n",
    "        for i in range(k):\n",
    "            for j in range(len(C[i])):\n",
    "                err += math.pow(norm(centroids[i]-p[C[i][j]]),2)\n",
    "                \n",
    "        if iter == counter:\n",
    "            print 'WARNING: Ran out of iterations'\n",
    "    return (centroids, err)\n",
    "\n",
    "\n",
    "def mykmeans_pp(p,k,iter):\n",
    "    centroids = []\n",
    "    centroids.append(p[random.randint(0, len(p)-1)])\n",
    "\n",
    "    for centroid_counter in range(1,k):\n",
    "        \n",
    "        # assign sample to cluster\n",
    "        C = get_buckets_of_samples(p,centroids)\n",
    "        \n",
    "        # construct probability distribution space\n",
    "        denomin = 0.0\n",
    "        for i in range(len(C)):\n",
    "            for j in range(len(C[i])):\n",
    "                denomin += math.pow(norm(centroids[i]-p[C[i][j]]),2)\n",
    "\n",
    "        prob_range = 0.0\n",
    "        for i in range(len(C)):\n",
    "            for j in range(len(C[i])):\n",
    "                prob_range += \\\n",
    "                        math.pow(norm(centroids[i]-p[C[i][j]]),2)/denomin\n",
    "        prob_range_max = prob_range\n",
    "        \n",
    "        # make random draw\n",
    "        random_point = random.uniform(0, prob_range_max)\n",
    "        \n",
    "        assert len(centroids)==centroid_counter\n",
    "\n",
    "        # map back to sample\n",
    "        prob_range = 0.0\n",
    "        done = False\n",
    "        for i in range(len(C)):\n",
    "            if done == False:\n",
    "                for j in range(len(C[i])):\n",
    "                    prob_range += \\\n",
    "                            math.pow(norm(centroids[i]-p[C[i][j]]),2)/denomin\n",
    "                    if random_point < prob_range:\n",
    "                        centroids.append(p[C[i][j]])\n",
    "                        done = True\n",
    "                        break\n",
    "                    if random_point >= prob_range and i==len(C)-1 and j==len(C[i])-1:\n",
    "                        print 'ERROR',random_point, prob_range_max, prob_range\n",
    "                        \n",
    "        assert len(centroids)==centroid_counter+1\n",
    "        \n",
    "    assert len(centroids) == k\n",
    "\n",
    "    return mykmeans(p,k,iter,np.asarray(centroids))\n",
    "\n",
    "\n",
    "def mykmeans_multi(p,k,iter,rep,method=None,center=False):\n",
    "    if center:\n",
    "        p_mean = p.mean(axis=0)\n",
    "        p -= p_mean\n",
    "    runs = []\n",
    "    errs = []\n",
    "    for r in range(rep):\n",
    "        if method == '++':\n",
    "            run = mykmeans_pp(p,k,iter)\n",
    "            centroids = run[0]\n",
    "            err = run[1]\n",
    "        else:\n",
    "            run = mykmeans(p,k,iter)\n",
    "            centroids = run[0]\n",
    "            err = run[1]\n",
    "\n",
    "        if center:\n",
    "            centroids += p_mean\n",
    "        runs.append(centroids)\n",
    "        errs.append(err)\n",
    "    return (runs[np.argmin(errs)], errs[np.argmin(errs)])\n",
    "\n",
    "\n",
    "# load iris data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# select particular class per assignment instructions\n",
    "X_temp=[]\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 0:\n",
    "        X_temp.append(X[i])\n",
    "X = np.asarray(X_temp)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "k=3\n",
    "max_iter=50\n",
    "rep=100\n",
    "\n",
    "start = time.time()\n",
    "print 'mykmeans',mykmeans(X.copy(),k,max_iter)\n",
    "print\n",
    "end = time.time()\n",
    "print 'Time:', end - start\n",
    "print\n",
    "\n",
    "start = time.time()\n",
    "print 'mykmeans_multi',mykmeans_multi(X.copy(),k,max_iter,rep)\n",
    "print\n",
    "end = time.time()\n",
    "print 'Time:', end - start\n",
    "print\n",
    "\n",
    "start = time.time()\n",
    "print 'mykmeans_pp',mykmeans_pp(X.copy(),k,max_iter)\n",
    "print\n",
    "end = time.time()\n",
    "print 'Time:', end - start\n",
    "print\n",
    "\n",
    "for center in [False, True]: # try both centered and non-centered data\n",
    "    start = time.time()\n",
    "    (centroid_list,err) = mykmeans_multi(X.copy(),k,max_iter,rep,method='++',\n",
    "                                         center=center)\n",
    "    print 'mykmeans_multi w/pp', centroid_list, err, center\n",
    "    print\n",
    "    end = time.time()\n",
    "    print 'Time:', end - start\n",
    "    print\n",
    "\n",
    "# base line\n",
    "start = time.time()\n",
    "km = KMeans(n_clusters=k,max_iter=max_iter,n_init=rep,n_jobs=1)\n",
    "km.fit(X.copy())\n",
    "end = time.time()\n",
    "\n",
    "# check accuracy of scikit learn\n",
    "\n",
    "# assign sample to cluster\n",
    "C = get_buckets_of_samples(X.copy(),km.cluster_centers_)\n",
    "assert len(C) == k\n",
    "err = 0\n",
    "for i in range(k):\n",
    "    for j in range(len(C[i])):\n",
    "        err += math.pow(norm(km.cluster_centers_[i]-X[C[i][j]]),2)\n",
    "\n",
    "print 'KMeans',km.cluster_centers_, err\n",
    "print\n",
    "print 'Time:', end - start\n",
    "\n",
    "%matplotlib nbagg\n",
    "centroid_list_2d = pca.transform(centroid_list)\n",
    "print centroid_list_2d\n",
    "\n",
    "print pca.transform(km.cluster_centers_)\n",
    "\n",
    "plt.scatter(X_2d[:,0],X_2d[:,1])\n",
    "for i in range(k):\n",
    "    plt.scatter(centroid_list_2d[i,0],centroid_list_2d[i,1],c='r',marker='*')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With Text Data\n",
    "See: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## KNeighborsClassifier\n",
    "Here we predict the category of news articles by training a nearest neighbor classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, \n",
    "                                  shuffle=True, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier().fit(X_train_tfidf, twenty_train.target)\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Sentiment Analysis on movie reviews\n",
    "Here we predict the sentiment of movie reviews by a training naive Bayes classifier. See: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#exercise-2-sentiment-analysis-on-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "%matplotlib nbagg\n",
    "\n",
    "movie_reviews_data_folder = 'txt_sentoken/'\n",
    "dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "\n",
    "# split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.50, random_state=8349)\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(min_df=.1,max_df=.9)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=2)\n",
    "gs_clf = gs_clf.fit(docs_train, y_train)\n",
    "for element in gs_clf.grid_scores_:\n",
    "    print element\n",
    "\n",
    "y_predicted = gs_clf.predict(docs_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "# Print and plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(cm)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Features\n",
    "We consider local features on the Newsgroup dataset. We use Bag of Words for each sample and then train Word2Vec from training data to get vectors from words. Then we cluster those vectors. \n",
    "\n",
    "To find the Bag of Features on unseen samples, we take the Bag of Words and use Word2Vec to convert to vectors. Then we find the nearest centroid for each vectorized word. The most common centroid then predicts the classification of the unseen sample.\n",
    "\n",
    "Note: Scikit-learn KMeans is much faster than our implementation however both converge to about the same error rate. If we used sparse matrices or Cython, we could also speed up our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import re, copy, time\n",
    "\n",
    "def learnvocabulary(data_orig):\n",
    "    # Clean data\n",
    "    data = copy.deepcopy(data_orig)\n",
    "    for i in range(len(data)):\n",
    "        data[i] = re.sub(\"[^a-zA-Z]\",\" \", data[i])\n",
    "        data[i] = data[i].lower().split()\n",
    "        \n",
    "    # Create Model to Convert to Words to Vectors\n",
    "    start = time.time()\n",
    "    w2v_model = Word2Vec(data,workers=4)\n",
    "    w2v_model.init_sims(replace=True) # use to reduce memory usage\n",
    "    vocab_vectors = w2v_model.syn0\n",
    "    end = time.time()\n",
    "    print 'Word2Vec Time:', end - start\n",
    "\n",
    "    # Create KMeans model to cluster word vectors\n",
    "    k=10\n",
    "    max_iter=9999999\n",
    "    rep=5\n",
    "    \n",
    "    start = time.time()\n",
    "    (centroid_list,err) = mykmeans_multi(vocab_vectors.copy(), k, max_iter,\n",
    "                                         rep, method='++', center=True)\n",
    "    end = time.time()\n",
    "    print 'mykmeans_multi w/pp Time:', end - start\n",
    "    print 'mykmeans_multi w/pp err:', err\n",
    "    \n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.cluster_centers_ = centroid_list\n",
    "\n",
    "\n",
    "# Baseline    \n",
    "#     start = time.time()\n",
    "#     km = KMeans(n_clusters=k,n_init=rep,n_jobs=4)\n",
    "#     km.fit(vocab_vectors)\n",
    "#     end = time.time()\n",
    "#     print 'KMeans Time:', end - start\n",
    "#     # check accuracy of scikit learn\n",
    "#     # assign sample to cluster\n",
    "#     C = get_buckets_of_samples(vocab_vectors.copy(),km.cluster_centers_)\n",
    "#     assert len(C) == k\n",
    "#     err = 0\n",
    "#     for i in range(k):\n",
    "#         for j in range(len(C[i])):\n",
    "#             err += math.pow(norm(km.cluster_centers_[i] - \\\n",
    "#                                  vocab_vectors[C[i][j]]),2)\n",
    "#     print 'KMeans err:', err\n",
    "\n",
    "\n",
    "\n",
    "    return w2v_model, km\n",
    "\n",
    "\n",
    "def getbof(sample, w2v_model, kmeans_model):\n",
    "    sample_copy = copy.deepcopy(sample)\n",
    "    sample_copy = re.sub(\"[^a-zA-Z]\",\" \", sample_copy)\n",
    "    sample_copy_split = sample_copy.lower().split()\n",
    "\n",
    "    sample_vector = []\n",
    "    for s in sample_copy_split:\n",
    "        try:\n",
    "            sample_vector.append(w2v_model[s])\n",
    "        except KeyError: # Ignore unseen words\n",
    "            pass\n",
    "    \n",
    "    # predict nearest feature for each word\n",
    "    centroid_predictions = []\n",
    "    for s in sample_vector:\n",
    "        centroid_predictions.append(kmeans_model.predict(s))\n",
    "\n",
    "    for i in range(len(centroid_predictions)):\n",
    "        centroid_predictions[i] = \\\n",
    "                    kmeans_model.cluster_centers_[centroid_predictions[i]]\n",
    "        \n",
    "    return centroid_predictions\n",
    "    \n",
    "# Get data\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories = ['alt.atheism'],\n",
    "                                  shuffle=True, random_state=9876)\n",
    "\n",
    "# Get vocab via feature (word) centroids\n",
    "w2v_model, kmeans_model = learnvocabulary(twenty_train.data)\n",
    "\n",
    "# Get feature centroid for local features (centroid for each word)\n",
    "bof = getbof(twenty_train.data[0], w2v_model, kmeans_model)\n",
    "print bof\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--bibtex\n",
    "\n",
    "@Article{Joachims,\n",
    "  Author    = {T. Joachims},\n",
    "  Title     = {Training linear SVMs in linear time},\n",
    "  Journal   = {ACM Conference on Knowledge Discovery and Data Mining (KDD)},\n",
    "  year      = 2006,\n",
    "}\n",
    "\n",
    "@article{Papa2007,\n",
    "  author = {Papa, David A. and Markov, Igor L.},\n",
    "  journal = {Approximation algorithms and metaheuristics},\n",
    "  pages = {1--38},\n",
    "  title = {{Hypergraph partitioning and clustering}},\n",
    "  url = {http://www.podload.org/pubs/book/part\\_survey.pdf},\n",
    "  year = {2007}\n",
    "}\n",
    "-->\n",
    "\n",
    "# Random Features for Large-Scale Kernel Machines\n",
    "http://www.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf\n",
    "## Introduction\n",
    "Kernel machines such as Support Vector Machines (SVM) operate on the kernel matrix (Gram matrix). The kernel matrix is composed of every combination of the transformed points' dot product. So the elements of the kernel matrix grow with the square of the data. Because of limited computational power, this limits the amount of data that can be use to train a generic kernel machine. However, linear Support Vector Machines and regularized regression operate on the covariance matrix instead of the kernel matrix. Linear SVMs can be trained in linear time [CITATION](#cite-Joachims). Proposed is a way to combine the advantages of the linear and nonlinear approaches. The data is mapped to random features in lower dimensional space and then a linear learning technique is applied. By the kernel trick, the kernel of samples x and y is just the dot product of a transformation of x and y.\n",
    "\n",
    "Let $k$ be kernel that is a positive definite function and x, y corresponding to samples in $R^d$\n",
    "$$ k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle $$\n",
    "Now define mapping $z : R^d \\rightarrow R^D$ where D << d\n",
    "With a linear kernel,\n",
    "$$ k(z(x),z(y)) = \\langle z(x), z(y) \\rangle = z(x)^T z(y)$$\n",
    "By a using randomized features transformation,\n",
    "$k(x,y) \\approx z(x)^T z(y)$.\n",
    "Analytical and empirical results are shown.\n",
    "\n",
    "When evaluating a sample against a kernel machine, we compute $f(x) = \\sum^N_{i=1}c_i k(x_i,x)$, which requires $O(Nd)$. But, using a linear kernel and finding hyperplane $w$ in $R^D$, we compute $f(x) = w^Tz(x)$, which is only $O(D+d)$, using the proposed randomized feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Binning Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy.random import *\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def z(x,P):\n",
    "    z=np.zeros(P)\n",
    "    for p in range(1,P+1):\n",
    "        d = len(x)\n",
    "        # randomly pick u and delta\n",
    "        delta = random_integers(1,p)\n",
    "        u = rand(d)*delta\n",
    "\n",
    "        to_hash = np.ceil((x-u)/delta)\n",
    "        to_hash.flags.writeable = False\n",
    "        z[p-1] = hash(to_hash.data)\n",
    "    return math.sqrt(1.0/P)*z\n",
    "\n",
    "\n",
    "a = np.asarray([1,2,3,4,5,6,7,8,9,10,11,12,13])\n",
    "b = np.asarray([13,12,11,10,9,8,7,6,5,4,3,2,1])\n",
    "z_a = z(a,5)\n",
    "z_b = z(b,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
