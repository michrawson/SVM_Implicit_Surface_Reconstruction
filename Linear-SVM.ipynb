{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Loss vs. Hinge Loss vs. Huberized Hinge Loss vs. Square Hinge Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "x_plot=np.linspace(-2,2,1000)\n",
    "y_plot1=x_plot.copy()\n",
    "y_plot1[x_plot < 0]=1\n",
    "y_plot1[x_plot == 0]=0\n",
    "y_plot1[x_plot > 0]=0\n",
    "plot1 = ax.plot(x_plot,y_plot1, label='Classification Loss')\n",
    "\n",
    "y_plot2=np.maximum(np.zeros(x_plot.shape),1-x_plot.copy())\n",
    "plot2 = ax.plot(x_plot,y_plot2, label='Hinge Loss')\n",
    "\n",
    "y_plot4=np.power(np.maximum(np.zeros(x_plot.shape),1-x_plot.copy()),2)\n",
    "plot4 = ax.plot(x_plot,y_plot4, label='Square Hinge Loss')\n",
    "\n",
    "h=.8\n",
    "y_plot3= -1 * np.ones(x_plot.shape)\n",
    "y_plot3[x_plot > 1+h]=0\n",
    "y_plot3[x_plot < 1-h]=1-x_plot[x_plot < 1-h]\n",
    "y_plot3[y_plot3 == -1]= ((1+h-x_plot[y_plot3 == -1])**2)/(4*h)\n",
    "plot3 = ax.plot(x_plot,y_plot3, label='Huberized Hinge Loss')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Loss Comparison')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Distance From Decision Boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic Expressions\n",
    "\n",
    "Let $X \\in R^{n \\times d+1}$ and $y = (y_1,...,y_n)^T \\in R^{n+1}$ and $\\texttt{loss}(...) \\ge 0$\n",
    "\n",
    "Objective function: \n",
    "$$F(w) = \\|w\\|^2 + \\frac Cn \\|\\texttt{loss}(y,Xw)\\|_1$$\n",
    "\n",
    "Clearly,\n",
    "$$(\\vec\\nabla F(w))_j = 2w_j + \\frac Cn \\sum^{n}_{i=1}\\frac{d}{d(Xw)_i}\\texttt{loss}(y_i,(Xw)_i) \\cdot X_{i,j} \\quad \\texttt{for} ~j = 1,2,...,d+1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For Hinge Loss:\n",
    "$$l_{hinge}(y,t) := \\max(0, 1 - yt)$$\n",
    "Then\n",
    "$$\\frac{d}{dt}l_{hinge}(y,t) := \\begin{cases} 0, & \\mbox{if } 1-yt \\lt 0\\\\ \n",
    "-y, & \\mbox{if } 1-yt \\gt 0  \\end{cases}$$\n",
    "And\n",
    "$$F(w) = \\|w\\|^2 + \\frac Cn \\sum^n\\max(0, 1 - y*(Xw))$$\n",
    "And $\\texttt{for j = 1,2,...,d+1}$\n",
    "$$(\\vec\\nabla F(w))_j = \\begin{cases} 2w_j, & \\mbox{if } 1-y*(Xw) \\lt 0\\\\\n",
    "2w_j + \\frac Cn \\sum^{n}_{i=1} -y \\cdot X_{i,j}, & \\mbox{if } 1-y*(Xw) \\gt 0  \\end{cases}$$\n",
    "Where $y*(Xw) \\in R^{n}$<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For Square Hinge Loss:\n",
    "$$l_{square-hinge}(y,t) := \\max(0, 1 - yt)^2$$\n",
    "Then\n",
    "$$\\frac{d}{dt}l_{square-hinge}(y,t) := \\begin{cases} 0, & \\mbox{if } 1-yt \\lt 0\\\\ \n",
    "2(1 - yt)(-y), & \\mbox{if } 1-yt \\gt 0  \\end{cases}$$\n",
    "And\n",
    "$$F(w) = \\|w\\|^2 + \\frac Cn \\sum^n\\max(0, 1 - y*(Xw))^2$$\n",
    "And $\\texttt{for j = 1,2,...,d+1}$\n",
    "$$(\\vec\\nabla F(w))_j = \\begin{cases} 2w_j, & \\mbox{if } 1-y*(Xw) \\le 0\\\\\n",
    "2w_j + \\frac Cn \\sum^{n}_{i=1} 2(1 - y*(Xw))(-y) \\cdot X_{i,j}, & \\mbox{if } 1-y*(Xw) \\gt 0  \\end{cases}$$\n",
    "Where $y*(Xw) \\in R^{n}$<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For Huberized Hinge Loss:\n",
    "$$l_{huber-hinge}(y,t) := \\begin{cases} 0, & \\mbox{if } yt \\gt 1+h\\\\ \n",
    "\\frac{(1+h-yt)^2}{4h}, & \\mbox{if } |1-yt| \\le h \\\\ \n",
    "1-yt, & \\mbox{if } yt \\lt 1-h \\end{cases}$$\n",
    "\n",
    "$$\\frac{d}{dt}l_{huber-hinge}(y,t) := \\begin{cases} 0, & \\mbox{if } yt \\gt 1+h\\\\ \n",
    "\\frac{2(1+h-yt)(-y)}{4h}, & \\mbox{if } |1-yt| \\le h \\\\ \n",
    "-y, & \\mbox{if } yt \\lt 1-h \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have continuity in $\\frac{d}{dt}l_{huber-hinge}(y,t)$ since the derivatives limits' agree at the critical points;\n",
    "$$ \\lim_{t^+ \\rightarrow \\frac{1+h}{y} } \\frac{d}{dt}l_{huber-hinge}(y,t) = 0$$\n",
    "$$ \\lim_{t^- \\rightarrow \\frac{1+h}{y} } \\frac{d}{dt}l_{huber-hinge}(y,t) = \\lim_{t^- \\rightarrow \\frac{1+h}{y} } \\frac{2(1+h-yt)(-y)}{4h} = \\lim_{t^- \\rightarrow \\frac{1+h}{y} } \\frac{2(1+h-y\\frac{1+h}{y})(-y)}{4h} = 0$$\n",
    "So\n",
    "$$ \\lim_{t^+ \\rightarrow \\frac{1+h}{y} } \\frac{d}{dt}l_{huber-hinge}(y,t) = \\lim_{t^- \\rightarrow \\frac{1+h}{y} } \\frac{d}{dt}l_{huber-hinge}(y,t)$$\n",
    "And\n",
    "$$ \\lim_{t^+ \\rightarrow \\frac{1-h}{y} } \\frac{d}{dt}l_{huber-hinge}(y,t) = \\lim_{t^+ \\rightarrow \\frac{1-h}{y} } \\frac{2(1+h-yt)(-y)}{4h}  = \\lim_{t^+ \\rightarrow \\frac{1-h}{y} } \\frac{2(1+h-y\\frac{1-h}{y})(-y)}{4h} = -y$$\n",
    "$$\\lim_{t^- \\rightarrow \\frac{1-h}{y} } \\frac{d}{dt}l_{huber-hinge}(y,t) = \\lim_{t^- \\rightarrow \\frac{1-h}{y} } -y = -y $$\n",
    "So\n",
    "$$ \\lim_{t^+ \\rightarrow \\frac{1-h}{y} } \\frac{d}{dt}l_{huber-hinge}(y,t) = \\lim_{t^- \\rightarrow \\frac{1-h}{y} } \\frac{d}{dt}l_{huber-hinge}(y,t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note for Huberized Hinge Loss:\n",
    "$$F(w) = \\begin{cases} \\|w\\|^2, & \\mbox{if } y*(Xw) \\gt 1+h\\\\ \n",
    "\\|w\\|^2 + \\frac Cn \\sum^n_1 \\frac{(1+h-y*(Xw))^2}{4h}, & \\mbox{if } |1-y*(Xw)| \\le h \\\\ \n",
    "\\|w\\|^2 + \\frac Cn \\sum^n_1 1-y*(Xw), & \\mbox{if } y*(Xw) \\lt 1-h \\end{cases}$$\n",
    "Where $y*(Xw) \\in R^{n}$<br />\n",
    "And $\\texttt{for j = 1,2,...,d+1}$\n",
    "$$(\\vec\\nabla F(w))_j = \\begin{cases} 2w_j, & \\mbox{if } y*(Xw) \\gt 1+h\\\\ \n",
    "2w_j + \\frac Cn \\sum^n_1 \\frac{2(1+h-y*(Xw))(-y)}{4h}*(Xw), & \\mbox{if } |1-y*(Xw)| \\le h \\\\ \n",
    "2w_j + \\frac Cn \\sum^n_1 -yX_{ij}, & \\mbox{if } y*(Xw) \\lt 1-h \\end{cases}$$\n",
    "Where $y*(Xw) \\in R^{n}$<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import randint\n",
    "import random\n",
    "import math\n",
    "import time \n",
    "\n",
    "\n",
    "def loss_hinge(y,t): # const\n",
    "    return max(0,1-(y*t))\n",
    "\n",
    "\n",
    "def loss_hinge_der(y,t): # const\n",
    "    if 1-(y*t) <= 0:\n",
    "        return 0\n",
    "    return -y\n",
    "\n",
    "\n",
    "def loss_huber_hinge(y,t,h): # const\n",
    "    if y*t > 1+h:\n",
    "        return 0\n",
    "    if abs(1-(y*t)) <= h:\n",
    "        return pow((1+h-(y*t)),2)/(4*h)\n",
    "    if y*t < 1-h:\n",
    "        return 1 - (y*t)\n",
    "\n",
    "def loss_huber_hinge_der(y,t,h): # const\n",
    "    if y*t > 1+h:\n",
    "        return 0\n",
    "    if abs(1-(y*t)) <= h:\n",
    "        return 2*(1+h-(y*t))*(-y)/(4*h)\n",
    "    if y*t < 1-h:\n",
    "        return -y\n",
    "\n",
    "\n",
    "def loss(y,t,loss_type,h): # const\n",
    "    if loss_type=='hinge':\n",
    "        return loss_hinge(y,t)\n",
    "    if loss_type=='modified_huber':\n",
    "        return loss_huber_hinge(y,t,h)\n",
    "\n",
    "\n",
    "def loss_der(y,t,loss_type,h): # const\n",
    "    if loss_type=='hinge':\n",
    "        return loss_hinge_der(y,t)\n",
    "    if loss_type=='modified_huber':\n",
    "        return loss_huber_hinge_der(y,t,h)\n",
    "\n",
    "\n",
    "def compute_obj(w,C,X,y,loss_type,h): # const\n",
    "    ret = 0.0\n",
    "    assert len(X)==len(y)\n",
    "    assert len(X[0])==len(w)\n",
    "    for i in range(len(X)):\n",
    "        ret += loss(y[i],np.dot(X[i],w),loss_type,h)\n",
    "    return norm(w)**2 + C*ret/len(X)\n",
    "\n",
    "\n",
    "def compute_grad(w,C,X,y,loss_type,h): # const\n",
    "    if len(X)==len(w):\n",
    "        grad = 2*w.copy()\n",
    "        for i in range(len(w)):\n",
    "            grad[i] += C*(loss_der(y,np.dot(X,w),loss_type,h)*X[i])\n",
    "        return grad\n",
    "    if len(X)==len(y) and len(X[0])==len(w):\n",
    "        n=len(X)\n",
    "        X[n-1,len(w)-1]\n",
    "        grad = 2*w.copy()\n",
    "        for i in range(len(w)):\n",
    "            loss_sum = 0.0\n",
    "            for j in range(n):\n",
    "                loss_sum += \\\n",
    "                    loss_der(y[j],np.dot(X[j],w),loss_type,h)*X[j,i]\n",
    "            grad[i] += C/n*loss_sum\n",
    "        return grad\n",
    "    assert False\n",
    "\n",
    "\n",
    "def numer_grad(w,ep,delta,C,X,y,loss_type,h): # const\n",
    "    return (compute_obj(w+(ep*delta),C,X,y,loss_type,h) \\\n",
    "           -compute_obj(w-(ep*delta),C,X,y,loss_type,h))/(2*ep)\n",
    "\n",
    "\n",
    "def grad_checker(w0,C,X,y,loss_type,h): # const\n",
    "    ep=.0001\n",
    "    delta=0\n",
    "    d=len(w0)\n",
    "    w=[]\n",
    "    for i in range(d):\n",
    "        delta=np.zeros(w0.shape)\n",
    "        delta[i] = 1\n",
    "        w.append(numer_grad(w0,ep,delta,C,X,y,loss_type,h))\n",
    "    return np.asarray(w)\n",
    "\n",
    "\n",
    "def score(X, y, w): # const\n",
    "    error = 0.0\n",
    "    error_comp = 0.0\n",
    "    for i in range(len(X)):\n",
    "        prediction = np.sign(np.dot(w,X[i]))\n",
    "        if prediction == 1 and y[i] == 1:\n",
    "            error += 1\n",
    "        elif (prediction == -1 or prediction == 0) and y[i] == -1:\n",
    "            error += 1\n",
    "        else:\n",
    "            error_comp += 1\n",
    "    return 'correct',error/len(X), 'failed',error_comp/len(X)\n",
    "\n",
    "\n",
    "def my_gradient_descent(X,y,w0=None,initial_step_size=.1,max_iter=1000,C=1,\n",
    "                        loss_type=None,h=.01,X_test=None, y_test=None,stochastic=False,back_track=True): # const\n",
    "    tol=10**-4 # scikit learn default\n",
    "    if w0 == None:\n",
    "        w0 = np.zeros(len(X[0]))\n",
    "    if len(X) == 0:\n",
    "        return 'Error'\n",
    "    diff = -1\n",
    "    grad = -1\n",
    "    \n",
    "    w = w0\n",
    "    obj_array = []\n",
    "\n",
    "    training_error_array = []\n",
    "    training_error_array.append(score(X, y, w=w))\n",
    "\n",
    "    testing_error_array = []\n",
    "    testing_error_array.append(score(X_test, y_test, w=w))\n",
    "    \n",
    "    w_array = []\n",
    "    w_array.append(w.copy())\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "#         print 'i',i\n",
    "        obj=compute_obj(w,C,X,y,loss_type,h)\n",
    "#         print 'obj',obj\n",
    "        obj_array.append(obj)\n",
    "        w_p = w\n",
    "        if stochastic:\n",
    "            random_index = randint(1,len(X))\n",
    "            grad = compute_grad(w,C,X[random_index],y[random_index],loss_type,h)\n",
    "        else:\n",
    "            grad = compute_grad(w,C,X,y,loss_type,h)\n",
    "            assert norm(grad-grad_checker(w,C,X,y,loss_type,h)) < tol #10**-2\n",
    "#         print 'grad',grad\n",
    "        if norm(grad) < tol:\n",
    "            break\n",
    "        \n",
    "        step_size = initial_step_size\n",
    "        if back_track:\n",
    "            while obj < compute_obj(w - (step_size * grad),C,X,y,loss_type,h):\n",
    "                step_size = step_size/2.0\n",
    "                if step_size < .00000001:\n",
    "                    break\n",
    "#         print 'step_size',step_size\n",
    "        w += - step_size * grad\n",
    "#         print 'w',w\n",
    "\n",
    "        w_array.append(w.copy())\n",
    "        training_error_array.append(score(X, y, w=w))\n",
    "        testing_error_array.append(score(X_test, y_test, w=w))\n",
    "\n",
    "        if training_error_array[len(training_error_array)-1][1] > 0.99:\n",
    "            break\n",
    "        \n",
    "        diff = norm(w-w_p)\n",
    "    if norm(grad) > tol:\n",
    "        print 'Warning: Did not converge.'\n",
    "    return w, w_array, obj_array, training_error_array, testing_error_array\n",
    "\n",
    "\n",
    "def my_sgd(X,y,w0=None,step_size=.01,max_iter=1000,C=1,loss_type=None,h=.01,X_test=None, \n",
    "           y_test=None,stochastic=False): # const\n",
    "    return my_gradient_descent(X_train, y_train,w0=w0,\n",
    "                               loss_type=loss_type,\n",
    "                               max_iter=max_iter,\n",
    "                               h=h,C=C,X_test=X_test, \n",
    "                               y_test=y_test,\n",
    "                               stochastic=stochastic)\n",
    "    \n",
    "\n",
    "\n",
    "def my_svm(X_train, y_train,loss_type=None,max_iter=None,h=None,C=None,X_test=None, y_test=None, \n",
    "           stochastic=False): # const\n",
    "    w0=np.zeros(len(X_train[0]))\n",
    "    if stochastic:\n",
    "        w, w_array, obj_array, training_error_array, testing_error_array = my_sgd(X_train, y_train,w0=w0,\n",
    "                                                                                   loss_type=loss_type,\n",
    "                                                                                   max_iter=max_iter,\n",
    "                                                                                   h=h,C=C,X_test=X_test, \n",
    "                                                                               y_test=y_test,stochastic=True)\n",
    "    else:\n",
    "        w, w_array, obj_array, training_error_array, testing_error_array = \\\n",
    "                        my_gradient_descent(X_train, y_train,w0=w0, loss_type=loss_type, max_iter=max_iter, h=h,C=C,\n",
    "                                            X_test=X_test, y_test=y_test, stochastic=stochastic)\n",
    "    return w, w_array, obj_array, training_error_array, testing_error_array\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Gradient Descent Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "'''Generate 2 Gaussians samples with the same covariance matrix'''\n",
    "n, dim = 500, 2\n",
    "np.random.seed(0)\n",
    "C = np.array([[0., -0.23], [0.83, .23]])\n",
    "gap = 1\n",
    "X = np.r_[np.dot(np.random.randn(n, dim)+gap, C),\n",
    "          np.dot(np.random.randn(n, dim)-gap, C)]\n",
    "\n",
    "# append constant dimension\n",
    "X = np.column_stack((X, np.ones(X.shape[0])))\n",
    "\n",
    "y = np.hstack((-1*np.ones(n), np.ones(n)))\n",
    "\n",
    "assert len(X[y==-1])==len(y[y==-1]);assert len(X[y==1])==len(y[y==1]);assert len(X)==len(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                            test_size=0.5, random_state=20140210)\n",
    "\n",
    "assert len(X_train)>1;assert len(X_test)>1;assert len(X_train)==len(y_train);assert len(X_test)==len(y_test)\n",
    "\n",
    "max_iter=1000\n",
    "C=1.0\n",
    "\n",
    "for loss_type in ['modified_huber','hinge']:\n",
    "    for h_index in range(1,10):\n",
    "        h=(.1*(1.1**h_index))\n",
    "        print 'parameters: loss_type',loss_type,'h',h\n",
    "\n",
    "        w, w_array, obj_array, training_error_array, testing_error_array = my_svm(X_train, y_train,\n",
    "                                                                                  loss_type=loss_type,\n",
    "                                                                                  max_iter=max_iter,h=h,C=C,\n",
    "                                                                                  X_test=X_test, y_test=y_test,\n",
    "                                                                                  stochastic=False)\n",
    "        print 'Custom w =',w,' test score = ',score(X_test, y_test, w=w)\n",
    "\n",
    "        clf = SGDClassifier(loss=loss_type, penalty=\"l2\",alpha=1/C, fit_intercept=False)\n",
    "        clf.fit(X_train, y_train);        assert clf.intercept_ == 0\n",
    "        print 'SGDClassifier w = ',clf.coef_[0],' test score = ',score(X_test, y_test, \n",
    "                                                                       w=clf.coef_[0])\n",
    "        clf = LinearSVC( penalty=\"l2\",C=C, fit_intercept=False); clf.fit(X_train, y_train); assert clf.intercept_ == 0\n",
    "        print 'LinearSVC w = ',clf.coef_[0],' test score = ',score(X_test, y_test, w=clf.coef_[0])\n",
    "        print\n",
    "        print\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Stochastic Gradient Descent Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "'''Generate 2 Gaussians samples with the same covariance matrix'''\n",
    "n, dim = 2*(10**7), 2\n",
    "n, dim = 500, 2\n",
    "np.random.seed(0)\n",
    "C = np.array([[0., -0.23], [0.83, .23]])\n",
    "gap = 1\n",
    "X = np.r_[np.dot(np.random.randn(n, dim)+gap, C),\n",
    "          np.dot(np.random.randn(n, dim)-gap, C)]\n",
    "\n",
    "# append constant dimension\n",
    "X = np.column_stack((X, np.ones(X.shape[0])))\n",
    "\n",
    "y = np.hstack((-1*np.ones(n), np.ones(n)))\n",
    "\n",
    "assert len(X[y==-1])==len(y[y==-1]);assert len(X[y==1])==len(y[y==1]);assert len(X)==len(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=20140210)\n",
    "\n",
    "assert len(X_train)>1;assert len(X_test)>1;assert len(X_train)==len(y_train);assert len(X_test)==len(y_test)\n",
    "\n",
    "max_iter=1000\n",
    "C=1.0\n",
    "\n",
    "for loss_type in ['modified_huber','hinge']:\n",
    "    for h_index in range(1,10):\n",
    "        h=(.1*(1.1**h_index))\n",
    "        print 'parameters: loss_type',loss_type,'h',h\n",
    "        w, w_stoch_array, obj_stoch_array, training_error_stoch_array, testing_error_stoch_array = \\\n",
    "        my_svm(X_train, \n",
    "            y_train, loss_type=loss_type, max_iter=max_iter,h=h,C=C, X_test=X_test, y_test=y_test,stochastic=True)\n",
    "\n",
    "        print 'Custom w =',w,' test score = ',score(X_test, y_test, w=w)\n",
    "\n",
    "        clf = SGDClassifier(loss=loss_type, penalty=\"l2\",alpha=1/C, fit_intercept=False);clf.fit(X_train, y_train)\n",
    "        assert clf.intercept_ == 0\n",
    "        print 'SGDClassifier w = ',clf.coef_[0],' test score = ',score(X_test, y_test, w=clf.coef_[0])\n",
    "        \n",
    "        clf = LinearSVC( penalty=\"l2\",C=C, fit_intercept=False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        assert clf.intercept_ == 0\n",
    "        print 'LinearSVC w = ',clf.coef_[0],' test score = ',score(X_test, y_test, w=clf.coef_[0])\n",
    "        print\n",
    "        print\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "w_array = np.asarray(w_array)\n",
    "ax.scatter(w_array[:,0],w_array[:,1],marker='^',label='Non-Stochastic')\n",
    "\n",
    "w_stoch_array = np.asarray(w_stoch_array)\n",
    "ax.scatter(w_stoch_array[:,0],w_stoch_array[:,1],marker='*',label='Stochastic')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('First Two Dimensions of Hyperplane over iterations')\n",
    "plt.ylabel('w [1]')\n",
    "plt.xlabel('w [0]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "obj_array = np.asarray(obj_array)\n",
    "ax.scatter(range(1,len(obj_array)+1),obj_array,marker='^',label='Non-Stochastic')\n",
    "\n",
    "obj_stoch_array = np.asarray(obj_stoch_array)\n",
    "ax.scatter(range(1,len(obj_stoch_array)+1),obj_stoch_array,marker='*',label='Stochastic')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Objective Function over iterations')\n",
    "plt.ylabel('F (w)')\n",
    "plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "training_error_array = np.asarray(training_error_array)\n",
    "testing_error_array = np.asarray(testing_error_array)\n",
    "ax.scatter(range(1,len(training_error_array)+1),training_error_array[:,1],marker='^',label='training error')\n",
    "ax.scatter(range(1,len(testing_error_array)+1),testing_error_array[:,1],marker='*',label='testing error')\n",
    "\n",
    "training_error_stoch_array = np.asarray(training_error_stoch_array)\n",
    "testing_error_stoch_array = np.asarray(testing_error_stoch_array)\n",
    "ax.scatter(range(1,len(training_error_stoch_array)+1),training_error_stoch_array[:,1],marker='+',\n",
    "           label='stochastic training error')\n",
    "ax.scatter(range(1,len(testing_error_stoch_array)+1),testing_error_stoch_array[:,1],marker='x',\n",
    "           label='stochastic testing error')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, labels)\n",
    "plt.title('Classification Error over iterations')\n",
    "plt.ylabel('Classification Error')\n",
    "plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "# print w\n",
    "x_plot=[w[0], 0]\n",
    "y_plot=[w[1], 0]\n",
    "ax.plot(x_plot,y_plot)\n",
    "\n",
    "# print clf.coef_[0]\n",
    "x_plot=[clf.coef_[0][0], 0]\n",
    "y_plot=[clf.coef_[0][1], 0]\n",
    "ax.plot(x_plot,y_plot)\n",
    "\n",
    "ax.scatter((X[y==0])[:,0],(X[y==0])[:,1],marker='*')\n",
    "ax.scatter((X[y==1])[:,0],(X[y==1])[:,1],marker='^')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.title('Data, Scikit-learn Hyperplane, and Our Own Hyperplane')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
